% 波-粒（局部连接）二象性
% 波和连接同时考虑比单独一个精度更高
% 全局的波在局部形成共振

% Alex --> 神经场理论中的深度学习 -> 几何约束
% https://github.com/kwignb/RandomNeuralField/blob/main/src/models/networks.py

% 神经场介绍
%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


% 需要将bst目录下的sn-mathphys-num.bst复制到根目录下，否则参考文献为问号
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

\usepackage{subfigure}  % 使用子图（多个图凑九宫格）

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

%\bibliography{reference}

\begin{document}

% Geometric constraints on human brain function
% Structure constraints human brain function and artificial neural network
\title[Article Title]{Whole-body physics simulation of human locomotion}
% Structure Bias in Artificial Neural Network

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

\author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

%\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
	% 出发点：身体 影响 神经系统生成行为
	The body of an human infuences how its nervous system generates behaviour\cite{dickinson2000animals}. 
	% 建模的重要性：感觉运动需要
	Accurately modelling the neural control of sensorimotor behaviour requires an anatomically detailed biomechanical representation of the body. 
	Here we introduce a whole-body model of the human in a physics simulator. 
	Designed as a general-purpose framework, our model enables the simulation of diverse movement behaviours, including both walking and running locomotion. 
	We validate its versatility by replicating realistic walking and running behaviours. 
	To support these behaviours, we develop phenomenological models for fuid and adhesion forces. 
	Using data-driven, end-to-end reinforcement learning\cite{peng2018deepmimic,hasenclever2020comic}, we train neural network controllers capable of generating naturalistic locomotion\cite{muijres2014flies,muijres2015body,deangelis2019manifold} along complex trajectories in response to high-level steering commands. 
	Furthermore, we show the use of visual sensors and hierarchical motor control\cite{merel2019hierarchical}, training a high-level controller to reuse a pretrained low-level fight controller to perform visually guided walking tasks. 
	Our model serves as an open-source platform for studying the neural control of sensorimotor behaviour in an embodied context.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

%\keywords{Whole-body physics simulation of human locomotion}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

% 识别人脸的人工神经网络如何用波动力学解释
\section{Introduction}\label{sec1}

% 大脑、身体、环境 -> (感觉运行循环) -> 行为
Human behaviour emerges from sensorimotor feedback loops that integrate signals from the brain, body and environment\cite{dickinson2000animals,peng2018deepmimic,hasenclever2020comic,muijres2014flies,muijres2015body,deangelis2019manifold,merel2019hierarchical,todorov2012mujoco}.
% 神经运动命令 -> （身体） -> 移动  -> （感觉反馈）
The body determines how neural motor commands translate into movement and how sensory feedback is generated in response. 
Therefore, a detailed biomechanical understanding of the body is crucial for modelling the neural control of movement.
Here we introduce a physics-based simulation framwork for an anatomically detailed model of the human Dong, designed to supported the modelling of diverse sensorimotor behaviours.
% 通过 RL 来演示行走和跑步
We validate our model by demonstrating realistic locomotion—both walking and running—using reinforcement learning (RL). 
This general-purpose simulation provides a platform for future studies of brain–body interactions across a broad range of human behaviours.


% 基于的其他相关工作
% 蠕虫、水螅
Our work follows previous physics-based models of the worm\cite{boyle2012gait}, hydra\cite{wang2023complete}, rodent\cite{merel2019deep} and fruit fly\cite{reiser2004vision,dickson2008integrative,lobato2022neuromechfly,wang2023neuromechfly,melis2024machine}. 
The Grand Unified Fly\cite{dickson2008integrative} pioneered sensorimotor closed-loop visually guided flight using a simplified body model and hand-designed controller. 
More recent work has revealed the basis of muscle actuation of the wing hinge\cite{melis2024machine}. 
In parallel, NeuroMechFly\cite{lobato2022neuromechfly,wang2023neuromechfly} introduced an anatomically detailed fruit fly model capable of walking and grooming, pairing a heuristically designed low-level walking controller with a learnt high-level controller to generate sensory-guided behaviours\cite{wang2023neuromechfly}.


% 自己的建模方法
Our work unifies running and walking in a single physics-based model, enhancing realism in body mechanics, physics interactions and control. 
We developed an anatomically detailed fly body model in the open-source PhysX physics engine, incorporating high-resolution imaging to reconstruct a male Dong (Fig.~\ref{fig:fig_1}). 
To accurately simulate both walking and running, we introduced a computationally efficient phenomenological fluid dynamics model to approximate aerodynamic forces from wing flapping and adhesion actuators to model foot–surface interactions.


Using high-speed kinematic tracking\cite{branson2009high,pereira2022sleap}, we trained closed-loop RL controllers capable of replicating naturalistic running movements. 
These controllers, trained for both walking (Fig.~\ref{fig:fig_2}) and running (Fig.~\ref{fig:fig_3}), operate using only high-level steering commands. 
Finally, we demonstrate the reuse of a pretrained low-level flight controller for vision-guided flight tasks (Fig.~\ref{fig:fig_4}). 
% 其他的动作：搬箱子
Through inverse kinematics, we further show that our model supports a broad behavioural repertoire beyond locomotion, including lifting a box.



%\section{Results}\label{sec2}
%
%Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\section{Results}\label{sec3}

		

\subsection{Body model geometry}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1.0\textwidth]{fig/fig_1.pdf}
	\caption{
		\textbf{Constructing the male human body model.
		}
		\textbf{(a)} Compilation of six parts representing a single human. 
		% 带腹部的胸部
		Maximum intensity projections of confocal stacks showing head, thorax with abdomen, wings and legs. 
		Scale bar, 1 mm.
		% 共角的体积 confocal volume
		% 低多边形 网格
		% 高多边形 网格
		% 关节
		% femur: 大腿骨
		% tibia: 胫节
		% tarsal: 跗骨
		\textbf{(b)} Left, a partial projection of the midleg confocal volume with the joints between the femur, tibia and tarsal segments indicated. 
		Middle, a 3D mesh extracted from the volume. 
		Right, a low-polygon leg model. 
		Scale bar, 0.2 mm.
		% 被分解的
		\textbf{(c)} An exploded low-polygon human model (around 20,000 faces) showing all body segments. 
		Scale bar, 1 mm.
		\textbf{(d)} The geometric human model assembled in Blender.
		\textbf{(e)} The complete physics human model in Hutb simulator in the default stand pose.
		\textbf{(f)} Human model in a running pose with retracted legs.
		% 透明视图
		% DoFs: 自由度
		\textbf{(g)} DoFs. Translucent bottom view with light-blue arrows indicating hinge joint axes pointing in the direction of positive rotation. 
		% 三个铰合形成 球形关节
		Groups of three hinge joints effectively form ball joints. 
		Cube: 6-DoF free joint required for free CoM motion in the simulator and is not a part of human's internal DoFs.
		\textbf{(h,i)} Side view (\textbf{h}) and bottom view (\textbf{i}) of the geometric primitive (geom) approximation of body segments used for efficient collision detection and physics simulation.
		Blue, collision detection geoms; 
		purple, geoms that have associated adhesion actuators; 
		orange, wing ellipsoid geoms for simulating flight with the advanced fluid force model.
		% contact force: 接触力
		% joint actoators: 关节执行机构
		\textbf{(j)}, Visualization of actuator forces generated when the model fly hangs upside down. 
		The adhesion actuators of the front-right, middle-left and hind-right legs are actively gripping the ceiling (orange); 
		the labrum (mouth) adhesors are also active; 
		other actuators are inactive (white). 
		The arrows visualize net contact forces proportional and opposite to the applied adhesion forces.
		% abdominal abduction: 腹部外展
		\textbf{(k)}, Exaggerated posture showing the coordinated activation of the abdominal abduction and tarsal flexion actuators. 
		Abdominal joints and tarsal joints (yellow) are each coupled with a single tendon actuator that simultaneously actuates multiple DoFs.
	} \label{fig:fig_1}
\end{figure}

% confocal fluorescence microscopy: 共焦 荧光 显微镜检查
% TODO: 用什么设备来捕获人体的高精度图像
We used confocal fluorescence microscopy to capture high-resolution images of the entire adult male human body (Fig.~\ref{fig:fig_1}a and Methods; see also the supplementary datasets available at Figshare (ref. 20)). 
% 识别关节支点
Chitin staining facilitated segmentation of body structures and identification of joint pivot points (Fig.~\ref{fig:fig_1}b). 
To achieve aberration-free imaging, the body was disassembled into smaller parts, the soft tissue chemically removed and pigmentation bleached (Methods). 
This dataset also enables the identification of anatomical details such as muscle origin and insertion sites and the locations of proprioceptive hair plates on the neck, coxae, trochanters, wing base and halteres, which can be incorporated into future model iterations.

% 激活
% 在未训练的网络中有 类别 选择性偏好特征
\subsection{Modelling body physics} \label{sec:preferred}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/fig_2.pdf}
	\caption{
		\textbf{Preferred feature images of face-selective units in untrained networks.
		}
		\textbf{(a)} Measurements of preferred feature images (PFI) of target units in Conv5 from the reverse-correlation analysis~\cite{bonin2011local}.
		Bright and dark 2D Gaussian filters were generated at a random position as an input stimulus set.
		The PFI was obtained as the summation of stimuli weighted by the corresponding response.
		The initial preferred feature image was calculated from the local Gaussian stimulus set by the classical reverse-correlation method~\cite{bonin2011local}.
		% RNN?
		Then, a new stimulus set was generated as the summation of the obtained PFI and local Gaussian stimuli, with the second preferred feature image then obtained from a new stimulus set.
		This procedure was repeated to obtain the preferred feature image.
		\textbf{(b)} Schematics of the process used to achieve a preferred feature image (PFI) using a generative neural network (GAN) and a genetic algorithm (X-Dream)~\cite{ponce2019evolving}.
		Synthesized images are generated by the GAN with image codes and are fed into an untrained network as input.
		The genetic algorithm finds a new image code that maximizes the response of the target unit.
		The PFI of a target unit is achieved after 100 iterations of this procedure.
		\textbf{(c)} The obtained preferred feature images, using the reverse-correlation method and X-Dream, of the face-selective unit, selective units to non-face class (flower), and units selective to none of the class.
		\textbf{(d)} Illustration of the face-configuration index (FCI) of a face unit's PFI.
		The FCI was defined as pixel-wise correlation between the original face stimuli and the generated PFIs.
		\textbf{(e)} FCI of PFI, using the reverse correlation method, of units selective to each class ($ n_\textrm{Face} = 465 $, $ n_\textrm{Hand} = 7 $, $ n_\textrm{Horn} = 772 $, $ n_\textrm{Flower} = 107 $, $ n_\textrm{Chair} = 63 $).
		\textbf{(f)} FCI of PFI, using X-Dream, of the same units as the units used in (\textbf{e}).
		All box plots indicate the inter-quartile range (IQR between Q1 and Q3) of the dataset,
		the horizontal line depicts the median and the whiskers correspond to the rest of the distribution (Q1-1.5*IQR, Q3+1.5*IQR).
		The face images shown in panels (\textbf{d})-(\textbf{f}) are selected examples from the publicly available dataset~\cite{stigliani2015temporal}.
		The original images are available at [\url{http://vpnl.stanford.edu/fLoc}].
	} \label{fig:fig_2}
\end{figure}


Next, to characterize the feature-selective responses of these face-selective units qualitatively,
we reconstructed preferred feature images (PFI) of individual units using a reverse-correlation (RC) method~\cite{bonin2011local} and a generative adversarial network algorithm (X-Dream)~\cite{ponce2019evolving} (see~\ref{sec:preferred}).
In the RC analysis, we presented 2500 images of bright and dard 2D Gaussian filters at random positions as input stimuli to an untrained network (\ref{fig:preferred}).
By adding stimuli weighted according to the corresponding neural response with 100 repeated iterations, we obtained the preferred feature images of the target units.
In the X-Dream analysis, a deep generative adversarial neural network~\cite{dosovitskiy2016generating} was trained to ImageNet datasets to synthesize preferred feature images from the image codes scored by a genetic algorithm using the responses of units from repeated iterations. (\ref{fig:preferred}b).
We found that the response of target units induced by the PFI was increased, being higher that those induced by face stimulus images, as the iteration number of the genetic algorithm exceeds a certain value 
(\ref{fig:preferred}a, right, $ n=465 $, 
RC PFI vs. face stimulus, two-sided rank-sum test, $ P=1.51 \times 10^{-6} $, $ r_{rbc} = 1.58 \times 10^{-1} $;
RC PFI vs. non-face stimulus, two-sided rank-sum test, $ P=3.12 \times 10^{-2} $, $ r_{rbc} = 7.07 \times 10^{-2} $,
\ref{fig:preferred}b, right, $ n=465 $, 
X-Dream PFI vs. face stimulus, two-sided rank-sum test, $ P=1.92 \times 10^{-119} $, $ r_{rbc} = 7.63 \times 10^{-1} $;
X-Dream PFI vs. non-face stimulus, two-sided rank-sum test, $ P = 2.68 \times 10^{-129} $, $ r_{rbc} = 7.94 \times 10^{-1} $
).
These results indicate that our PFI generation methods successfully find the most preferred input feature of face-selective units.
As a result, using both methods, we obtained the PFI of face-selective units, units selective to non-face classes, and units without selective responses to any image classes (\ref{fig:preferred}c).
We found that the PFI of the face-selective units presents distinguishable features from those of other objects.
We observed that the PFIs of face-selective units represent face-like configurations, whereas the PFIs of units selective to non-face classes show noticeable configurations of each object class (i.e., flowers in an RC and X-Dream).


% d
To quantify the structural similarity of the PFIs to face images, we defined the face-configuration index as the averaged pixel-wise correlation between a PFI and the 200 face images used for face unit selection (\ref{fig:preferred}d).
We estimated the face-configuration index of each PFI of face-selective and non-face-selective units generated by the RC method.
We found that the estimated PFIs of face-selective units (with a visually observable face-like configuration) have a significantly higher average value of the index than that from the units selective to non-face objects (\ref{fig:preferred}e, 
Face PFI vs. Non-face PFI, 
$ n_\textrm{Face}    = 465 $,
$ n_\textrm{Horn}    = 772 $,
$ n_\textrm{Hand}    = 7   $,
$ n_\textrm{Chair}   = 63  $,
$ n_\textrm{Flower}  = 107 $,
one-sided rank-sum test,
$ P \leq 5.63 \times 10^{-4} $,
$ r_{rbc} \geq 1.56 \times 10^{-1} $;
\ref{fig:preferred}f, Face PFI vs. Non-face PFI,
$ n_\textrm{Face}   = 465 $,
$ n_\textrm{Chair}  = 63  $,
$ n_\textrm{Horn}	= 773 $,
$ n_\textrm{Hand}	= 7	  $,
$ n_\textrm{Flower}	= 107 $,
one-sided rank-sum test, 
$ P \leq 1.93 \times 10^{-5} $,
$ r_{rbc} \geq 2.35 \times 10^{-1} $
).
Notably, the average pairwise correlation estimated between each face stimulus image shows a significantly lower value of nearly zero, 
implying that the observed index of face-selective units reflects structural similarity to the averaged (or a prototype, abstract) face image rather than similarly to particular face images accidentally observable.


Next, we hypothesized that the observed face-selective units may encode invariant representations of the prototype face images, as some types of intrinsic are a basic property of CNNs.
A number of previous studies suggested that various types of invariance (e.g., translation, scaling, and rotation) over a wide range of image transformations can be implemented in a CNN~\cite{lecun2004learning,kavukcuoglu2010learning,lecun2012learning,chidester2018rotation,srivastava2018effect}, mostly due to three key components--the convolutional layer, the pooling layer, and the hierarchical structure--in CNN models.
Thus, to investigate whether the observed face-selective units show invariant representations of face images regardless of corresponding image condition, 
we measured the response of face-selective units to face and non-face object images with various positions, sizes, and rotation angles.
First, we observed that single face units show constant face tuning under a fairly wide range of size/potion/rotation variations and that range was comparable with those of face-selective neurons in IT~\cite{zoccolan2007trade}.
Notably, we found that our model units also show the inversion effect observed in monkeys~\cite{tsao2006cortical,buiatti2019cortical,perrett1985visual}.
We found that the response of face-selective units to inverted face images are significantly lower that those to upright faces.


% 在随机网络中发现视角不变性的面部选择单元
Similarly, we found viewpoint-invariant face-selective units and mirror-symmetric viewpoint-specific units in a random network.
Interestingly, the number of viewpoint-invariant face-selective units increased along the network hierarchy, similar to previous observations in the brain~\cite{freiwald2010functional}.
These results show that the observed invariances can arise from the hierarchical structure with convolutional filtering without the contribution of structured spatial filters.
Notably, the current result also suggests a possible scenario through which to understand how viewpoint invariant selectivity can arise in infant animals.
From the similarities between the CNN and the biological brain models, i.e., that the fundamentals of both CNNs and sensory cortices are based on the hierarchical feedforward structure and that the process of convolution via weight sharing in CNNs can be approximated by a biological model of periodic functional maps with hypercolumns in the visual cortex,
our result may inspire insight into how innate invariance can arise in infant animals.


% 几何模式限制神经网络激活
\subsection{Modelling body–environment interactions}\label{subsec2}

% 全局的波传到人脸识别区出现的模式响应 vs 人脸识别模式
% 人脸识别区域几何模式（外形）来重建 人脸识别区域 的激活
% 使用人脑的几何特征模式预测人工神经网络的激活
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/fig_3.pdf}
	\caption{\textbf{Reconstruction of artificial neural network activity with geometric eigenmodes.}
	\textbf{a}, Geometric eigenmodes are derived from the cortical surface mesh by solving the eigenvalue problem,$ \Delta \psi = -\lambda \psi $ (equation (\ref{eq:1})). 
	The modes $ \psi_1, \psi_2, \psi_3, ... \psi_N $ are ordered from low to high spatial frequency (long to short spatial wavelengths). 
	Negative, zero and positive values are coloured blue, white and red, respectively.
	\textbf{b}, Modal decomposition of brain activity data. 
	The example shows how a spatial map, $ y(r,t) $, at a given time, $ t $, can be decomposed as a sum of modes, $ \psi_j $, weighted by $a_j$.
	\textbf{c}, Left, we reconstruct task-evoked data using spatial maps of activation for a diverse range of stimulus contrasts. 
	Right, we reconstruct spontaneous activity by decomposing the spatial map at each time frame and generating a region-to-region FC matrix.
	\textbf{d}, Reconstruction accuracy of seven key HCP task-contrast maps (Supplementary Information~\ref{sec:sup_2_1}) and restingstate FC as a function of the number of modes. 
	Insets show cortical surface reconstructions, demonstrating the spatial scales relevant to the first 10, 100 and 200 modes corresponding to spatial wavelengths of approximately 120, 40 and 30 mm, respectively.
	\textbf{e}, Group-averaged empirical task-activation maps and reconstructions (recon.) obtained using 10, 100 and 200 modes of the seven key HCP task contrasts. 
	Black arrowheads indicate localized activation patterns that are more accurately reconstructed when using short-wavelength modes. 
	\textbf{f}, Group-averaged empirical resting-state FC matrices and reconstructions using 10, 100 and 200 modes.
	} \label{fig:fig_3}
\end{figure}

We first examine the degree to which geometric eigenmodes can explain diverse aspects of human neocortical activity. 
To derive the eigenmodes, we use a mesh representation of a population-averaged template of the neocortical surface (Fig.~\ref{fig:1}a and Derivation of cortical geometric eigenmodes in Methods). 
We then construct the Laplace– Beltrami operator (LBO) from this surface mesh, which captures local vertex-to-vertex spatial relations and curvature, and solve the eigenvalue problem,
\begin{equation} \label{eq:1}
	\nabla^2 \psi = \Delta\psi = -\lambda \psi,
\end{equation}
where $ \nabla $ is the gradient operator, $ \Delta $ is the LBO and $ \psi = \{\psi_1(r), \psi_2(r),...\} $ is the family of geometric eigenmodes with the corresponding family of eigenvalues $ \lambda = \{ \lambda_1, \lambda_2, ... \} $. 
The eigenvalues are ordered sequentially according to the spatial frequency or wavelength of the spatial patterns of each mode (Fig.~\ref{fig:1}a~and Extended Data Fig.~\ref{fig:extended_fig_1}), such that $ \psi_1 $ is the mode with the longest wavelength. 
The resulting eigenmodes are orthogonal, forming a complete basis set to decompose spatiotemporal dynamics unfolding on the cortex as a weighted sum of modes with varying wavelengths (Fig.~\ref{fig:1}b~and Modal decomposition of brain activity in Methods).
Unless otherwise specified, we use $ N=200 $ modes throughtout this study.


Using this decomposition we evaluate the accuracy of geometric eigenmodes in capturing both task-evoked and spontaneous brain activity (Fig.~\ref{fig:1}c) measured in 255 healthy individuals from the Human Connectome Project27 (HCP; HCP data in Methods and Supplementary Information 2). 
For task-evoked activity, we map 47 task-based contrasts drawn from seven different tasks representing distinct evoked activation patterns. 
We then reconstruct each individual’s activation map using an increasing number of modes up to a maximum of 200 (Fig.~\ref{fig:1}d). 
For spontaneous, task-free (so-called resting-state) activity, we reconstruct the spatial map of activity at each time frame and then generate a region-to-region functional coupling (FC) matrix, describing correlations of activity among 180 discrete brain regions per hemisphere28. 
To allow direct comparison between task-evoked and spontaneous recordings, we apply the same regional parcellation to the task-evoked data (Cortical parcellations in Methods). 
Finally, we quantify reconstruction accuracy by calculating the correlation between empirical and reconstructed task-evoked activation maps and spontaneous FC matrices (Fig.~\ref{fig:1}d-f).


We observe that reconstruction accuracy increases with an increasing number of modes across all task contrasts and in the resting state, with $ r \ge 0.38 $ already achieved using just $ N=10 $ modes (Fig.~\ref{fig:1}d). 
Large-scale modes are also differentially recruited across different tasks, suggesting that particular stimuli excite specific modes (Fig.~\ref{fig:1}e). 
Improvements in reconstruction accuracy become slow after ten modes, reaching $ r \geq 0.80 $ at approximately $ N=100 $ modes, with only incremental increases in reconstruction accuracy beyond this point. 
Beause the first 100 modes have wavelengths above around 40 mm (Supplementary Table~\ref{tab:spatial_wavelength}), 
and the inclusion of shorter-wavelength modes only refines reconstruction of localized patterns (arrowheads in Fig.~\ref{fig:1}e), our findings suggest that the data are predominantly comprised of spatial patterns with long spatial wavelengths (see next section for a more detailed analysis).


These results are consistent across all 47 HCP task contrasts (Supplementary Fig.~\ref{fig:supp_1}) and parcellations of varying resolutions (Supplementary Fig.~\ref{fig:supp_2}), but data parcellated at higher resolution require more modes to achieve high reconstruction accuracy due to the low-pass spatial filtering effect of coarser parcellations. 
Our results are also not affected by the use of a population-averaged cortical surface template (rather than individual-specific surfaces) to derive the geometric eigenmodes (Supplementary Figs.~\ref{fig:supp_3}-\ref{fig:supp_5}~and Supplementary Information~\ref{sec:individual_specific}). 
Together these findings indicate that cortical geometric eigenmodes form a compact representation that captures diverse aspects of task-evoked and spontaneous cortical activity. 
Moreover, they show that such activity is dominated by long-wavelength, large-scale eigenmodes.


We next test the hypothesis that geometric eigenmodes provide a more parsimonious and fundamental description of dynamics than eigenmodes derived from a graph-based connectome approximation. 
To this end we compare the reconstruction accuracy of geometric eigenmodes against three alternative connectome-derived eigenmode basis sets (see Fig.~\ref{fig:2}a~for a schematic). 
The first basis set is derived empirically from a connectome mapped with dMRI tractography at vertex resolution and thresholded to obtain a connection density of  0.10\%, as done previously29 (Derivation of connectome eigenmodes in Methods). 
The second basis set is derived from a connectome constructed synthetically according to a homogeneous stochastic wiring process governed by an exponential distance-dependent connection probability to mimic simple, EDR-like connectivity (Derivation of EDR eigenmodes in Methods). 
Because the connection densities of empirical and EDR connectomes differed, we evaluated a third basis set derived from the empirical connectome thresholded at 1.55\% to match the density of the EDR connectome. 
The connectome, EDR and density-matched connectome eigenmodes described above are derived from the graph Laplacian (a discrete counterpart of the LBO) of their respective connectivity matrices (Fig.~\ref{fig:2}b~and Extended Data Fig.~\ref{fig:extended_fig_1}).


To summarize, geometric eigenmodes account for the intrinsic curvature of the cortical surface and local vertex-to-vertex relations in the surface mesh; 
connectome eigenmodes do not consider curvature but capture local spatial relations between mesh vertices, along with short- and long-range connections measured with dMRI; 
and EDR eigenmodes account for the effect of a homogeneous, stochastic, distance-dependent connection rule without fully capturing the cortical geometry (Fig.~\ref{fig:2}a). 
Contrasting these different basis sets thus allows us to disentangle the contributions to brain dynamics of cortical geometry from structural connectivity.


Direct comparison of the reconstruction accuracy of these different basis sets shows that geometric eigenmodes consistently show the highest reconstruction accuracy across both spontaneous (Fig.~\ref{fig:2}c) and task-evoked (Fig.~\ref{fig:2}d) data. 
EDR eigenmodes perform nearly as well as geometric eigenmodes whereas connectome eigenmodes are the least accurate. 
This finding holds true regardless of the parcellation used (Extended Data Figs.~\ref{fig:extended_fig_2}~and~\ref{fig:extended_fig_3}), the specific connection density used to generate the connectome eigenmodes (Supplementary Figs.~\ref{fig:extended_fig_6}~and~\ref{fig:extended_fig_7}~and Supplementary Information~\ref{sec:thresholding_effect}) and whether we generate the connectome using a discrete regional parcellation rather than at vertex resolution (Supplementary Fig.\ref{fig:extended_fig_8} and Supplementary Information~\ref{sec:thresholding_effect}). 
We additionally find that geometric eigenmodes show stronger out-of-sample generalization than principal components of the functional data themselves (calculated via principal component analysis (PCA); Supplementary Fig.\ref{fig:supp_9}, Extended Data Fig.~\ref{fig:extended_fig_4} and Supplementary Information~\ref{sec:comparison_eigenmodes_derived}) and better performance than Fourier spatial basis sets (Extended Data Fig.\ref{fig:extended_fig_5}, Supplementary Information~\ref{sec:comparison_fourier}~and Comparisons with statistical basis sets in Methods~\ref{sec:sets_comparisons}).

% 长波主导神经网络激活
\subsection{Imitation learning of locomotion}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/fig_4.pdf}
	\caption{\textbf{Geometric eigenmodes benchmarked against connectome-based eigenmodes.}
	\textbf{a}, Schematic of the anatomical properties used to derive eigenmodes for cortical geometry, the connectome and the EDR connectome. 
	Geometric eigenmodes rely on local surface mesh information such as links (blue) between neighbouring surface mesh vertices (dots) and curvature. 
	Connectome eigenmodes rely on local links between mesh vertices (blue) and short- and long-range connections (magenta) reconstructed empirically from dMRI. 
	EDR eigenmodes rely on connections (red) generated from a stochastic wiring process in which the probability of connection between vertices exponentially decays as a function of their distance.
	\textbf{b}, Example connectome and EDR eigenmodes. 
	Negative, zero and positive values are coloured blue, white and red, respectively. 
	Despite some similarities, the spatial patterns of the modes are distinct from those derived using cortical geometry (compare with Fig.~\ref{fig:1}a).
	\textbf{c}, Reconstruction accuracy of resting-state FC matrices achieved by geometric, EDR and two variants of connectome eigenmodes: 
	one using a connectome as defined using previous methods\cite{naze2021robustness} and the other with the same connection density as the EDR connectome to allow fair comparison (for other densities see Supplementary Figs.~\ref{fig:supp_6}~and~\ref{fig:supp_7}).
	} \label{fig:fig_4}
\end{figure}

\subsubsection{Walking}


\subsubsection{Running}
% 肌肉驱动的人类跑步模拟：https://simtk.org/projects/runningsim/

Reconstructions of both spontaneous and task-evoked data with geometric eigenmodes show that the spatial organization of brain activity is dominated by patterns with spatial wavelengths of about 40 mm or longer (Fig.~\ref{fig:1}d–f). 
This result counters classical approaches to analysis of neuroimaging data, in which stimulus-evoked activations are mapped by thresholding statistical maps to identify focal, isolated areas of heightened activity. 
This classical approach rests on the assumption that focal loci represent discrete brain regions putatively engaged by the stimulus and that subthreshold activity in other regions is of negligible interest. 
The surprisingly long-wavelength content of task-activation data (Fig.~\ref{fig:1}d-e) suggests that classical procedures focus only on the tips of the iceberg and obscure the underlying spatially extended and structured patterns of activity evoked by the task (see Extended Data Fig.~\ref{fig:extended_fig_6}~for an explanation of the reasons involved). 
These observations accord with the theoretical predictions of NFT and previous analyses of task-evoked electroencephalography (EEG) signals\cite{robinson2001modal,wingeier2001spherical}.


Here we leverage the modal decomposition described in Fig.~\ref{fig:1}b to characterize the complete spatial pattern—the entire iceberg— of task-evoked activation. 
To this end we analyse the spatial power spectrum obtained using a geometric mode decomposition of group-averaged unthresholded activation maps from the 47 task contrasts in HCP\cite{van2013wu,gorgolewski2015neurovault} (Modal power spectra of task-evoked activation maps in Methods). 
As an independent replication, we also analyse 10,000 unthresholded activation maps from 1,178 independent experiments available in the NeuroVault repository\cite{van2013wu,gorgolewski2015neurovault}, thus providing a comprehensive picture of the diversity of stimulus-evoked activation patterns mapped in the human brain.


Despite the wide range of stimuli, paradigms and data-processing approaches used to acquire these activation maps, we observe that a large fraction of power in the maps is concentrated in the first 50 modes, corresponding to spatial wavelengths greater than around 60 mm (Fig.~\ref{fig:3}a; similar results are found separately for each of the key HCP task-contrast maps; Extended Data Fig.~\ref{fig:extended_fig_7}). 
Using surrogate data, we confirm that these findings cannot be explained by the spatial smoothing  induced by typical fMRI processing pipelines, which can filter out short-wavelength spatial patterns of activity (Extended Data Fig.~\ref{fig:extended_fig_8}~and Supplementary Information~\ref{sec:modal_power_spectra}). 
We further observe that incremental, sequential removal of long-wavelength modes has a much greater impact on reconstruction accuracy than removal of short-wavelength modes (Fig.~\ref{fig:3}b~and Contributions of long- and short-wavelength modes in Methods~\ref{sec:wavelength_contributions}). 
For instance, across the seven key HCP task contrasts, removal of the top 25\% long-wavelength modes (modes 1–50) yields a drop in reconstruction accuracy of around 40–60\% whereas removal of the top 25\% short-wavelength modes (modes 151–200) yields a drop of only around 2–4\% (Fig.~\ref{fig:3}b, insets). 
These results indicate that, on temporal and spatial scales accessible with fMRI, evoked cortical activity comprises large-scale, nearly brain-wide spatial patterns, challenging classical views that such activity should be described in terms of discrete, isolated and anatomically localized activation clusters.


% 波动力学连接和几何和功能
\subsection{Hierarchical vision-guided walking}


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/fig_5.pdf}
	\caption{
	} \label{fig:fig_5}
\end{figure}


Geometric eigenmodes of the cortex are obtained by solving the eigenvalue problem of the LBO, which is also known as the Helmholtz equation (equation (\ref{eq:1})). 
In physically continuous systems, the solutions of the Helmholtz equation correspond to the spatial projections of the solutions of a more general wave equation, such that the resulting eigenmodes inherently represent the vibrational patterns, or standing waves, of the system’s dynamics\cite{levy2006laplace}. 
This equivalence implies that the superior efficacy of geometric eigenmodes in the reconstruction of diverse patterns of brain activity results from a fundamental role of wave dynamics in shaping these patterns, as predicted by NFT. 
This prediction has been confirmed through models of EEG recordings21,35, but waves across the whole brain have only recently been observed in fMRI signals\cite{robinson2005multiscale,bolt2022parsimonious} and thus far lack a theoretical explanation. 
Here we use NFT and geometric eigenmodes to show that wave dynamics can provide a unifying account of diverse empirical and physiological phenomena observed at scales accessible with fMRI.


We model neural activity using an isotropic damped NFT wave equation without regeneration\cite{robinson1997propagation} (Fig.~\ref{fig:4}a~and NFT wave model in Methods~\ref{sec:NFT_model}). 
Under this model, activity propagates between points on the neocortex through their white-matter connectivity with a strength that decays approximately exponentially with distance (Supplementary Fig.~\ref{fig:extended_fig_10}~and Supplementary Information~\ref{sec:NFT}~and~\ref{sec:NFT_wave}). 
To simulate resting-state neural activity we use a white noise input to mimic unstructured stochastic fluctuations\cite{robinson2005multiscale} (Modelling resting-state dynamics in Methods~\ref{sec:modelling_resting}). 
We compare the performance of this simple wave model with a biophysically based neural mass model (balanced excitation–inhibition (BEI) model) that has been used extensively to understand resting-state fMRI signals\cite{deco2014local} (Fig.~\ref{fig:4}a~and Neural mass model in Methods~\ref{sec:neural_mass}). 
The neural mass model is closely aligned with the classical, connectome-centric view of brain function, representing dynamics as the result of interactions between neuronal populations in discrete anatomical regions, coupled according to an empirically measured connectome.


We first compare the efficacy of the two models in capturing distinct and commonly studied properties of spontaneous, task-free FC: namely, static pairwise FC (edge FC), static node-level average FC (node FC) and time-resolved dynamic properties of FC (FCD) (Modelling resting-state dynamics in Methods~\ref{sec:modelling_resting}). 
Across all FC-based benchmark measures, the wave model shows comparable or superior performance in reconstruction of empirical data relative to the neural mass model (Fig.~\ref{fig:4}b). 
The wave model also captures time-lagged properties\cite{raut2021global,bolt2022parsimonious,mitra2015lag} of empirical resting-state activity more accurately than the mass model (Extended Data Fig.~\ref{fig:extended_fig_9}~and Measuring time-lagged properties of resting-state dynamics in Methods~\ref{sec:dynamics_measurement}). 
This strong performance of the wave model is remarkable given its relative simplicity: the wave model only requires the geometry of the cortex (that is, the surface mesh) as input and includes one fixed parameter and one free parameter ($ r_s $) for fitting to data (Extended Data Fig.~\ref{fig:extended_fig_10}) whereas the neural mass model requires a dMRI-derived interregional anatomical connectivity matrix and comprises 15 fixed parameters and four free parameters (Supplementary Information~\ref{sec:mass_optimization}). 
These considerations indicate that wave dynamics offer a more accurate and parsimonious mechanistic account of macroscale, spontaneous cortical dynamics captured by fMRI.


We next consider stimulus-evoked cortical activity in the wave model. 
We analyse cortical responses to sensory stimulation of primary visual cortex (V1), because it elicits a well-defined hierarchy of regional cortical responses\cite{felleman1991distributed,chaudhuri2015large} (Modelling stimulus-evoked dynamics in Methods). 
A 1 ms pulse input to V1 yields a propagating wave of activity that rapidly splits along the dorsal and ventral visual processing streams (Fig.~\ref{fig:4}c (arrows) and Supplementary Video 1), consistent with the mainstream understanding of hierarchical visual processing\cite{goodale1992separate}. 
Remarkably, this result indicates that geometric constraints on travelling waves of evoked activity are sufficient for the segregation of the dorsal and ventral processing streams, which have traditionally been thought to be driven primarily by complex patterns of layer-specific connectivity\cite{felleman1991distributed,goodale1992separate,markov2014anatomy}. 
Furthermore, the temporal profile of evoked responses across the visual system follows a well-defined timescale hierarchy, with higher-order association areas showing peak responses that are delayed and prolonged compared with lower-order visual areas (Fig.~\ref{fig:4}d). 
These findings thus indicate that this hierarchical ordering, previously identified in experimental and modelling studies\cite{chaudhuri2015large,hasson2008hierarchy,murray2014hierarchy}, emerges naturally from waves of excitation propagating through the cortical medium. Critically, this hierarchical temporal ordering of areal responses strongly correlates with an independent anatomical measure of the cortical processing hierarchy based on non-invasive estimates of myeloarchitecture (T1-weighted (T1w) and T2-weighted (T2w) ratio)\cite{glasser2011mapping,gao2020neuronal}. 
This correlation is particularly strong within the visual processing hierarchy ($ r = -0.72 $, one-sided spin-test $ P $ value ($P_{spin}$) = 0.003; Fig.~\ref{fig:4}e) but is also present when considering all cortical areas ($ r = -0.44 $, $ P_{spin} = 0.037 $; Supplementary Fig.~\ref{fig:supp_11}). 
Together, our modelling results show how simple wave dynamics unfolding on the geometry of the cortex provide a unifying generative mechanism for capturing complex properties of spatiotemporal brain activity.


\subsubsection{Geometry constrains subcortical activity}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/fig_4.pdf}
	\caption{
	} \label{fig:4}
\end{figure}


Our analyses thus far have focused on the strong coupling of geometry and dynamics in the neocortex. 
We next investigate this coupling in non-neocortical areas, focusing on the thalamus, striatum and hippocampus, because these structures have geometries easily captured using MRI data and their functional organization has been extensively studied\cite{tian2020topographic}.


We first generalize our eigenmode analysis to three-dimensional (3D) volumes (Estimating the geometric eigenmodes of non-neocortical structures in Methods), yielding geometric eigenmodes that extend spatially through the three spatial dimensions of each structure. 
Next, to fully capture the macroscale functional organization of these non-neocortical regions, we apply a widely used manifold learning procedure to voxel-wise FC data to obtain the key functional gradients in each structure\cite{haak2018connectopic} (Mapping the functional organization of non-neocortical structures in Methods). 
These functional gradients describe the principal axes of spatial organization dictated by similarities in FC, thus representing the dominant modes of variation in functional organization, ordered according to the percentage of variance in FC similarity that they explain.


The spatial profiles of the first three functional gradients of the thalamus, striatum and hippocampus (accounting for 24, 50 and 47\% of the variance in FC similarity, respectively) show a near-perfect match to the first three geometric eigenmodes (Fig.~\ref{fig:5}a–c; spatial correlations $ r\geq0.93 $). 
This tight correspondence generalizes out to the first 20 modes and first 20 gradients of each structure (with the first 20 gradients respectively accounting for 49, 70 and 68\% of total variance in FC similarity), with all absolute spatial correlations $ |r| > 0.5 $, except for the 20th gradient and 20th mode in the striatum and hippocampus (Fig.~\ref{fig:5}d–f). 
This strong relationship is striking given that the functional gradients are generated via a complex processing pipeline applied to fMRI-derived FC measures whereas the eigenmodes are derived simply from each structure’s geometry, independent of the functional data. These findings suggest that the functional organization of non-neocortical structures derives directly from their geometric eigenmodes.


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/fig_5.pdf}
	\caption{
	} \label{fig:5}
\end{figure}



%\subsection{Detection of face images using the responses of face-selective units}
%
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.9\textwidth]{fig/face_3.pdf}
%	\caption{
%		\textbf{Detection of face images using the response of face units in untrained networks.
%		}
%		(\textbf{a}) Design of the face detection task and SVM classifier using the responses of the untrained AlexNet.
%		During this task, face or non-face images were randomly presented to the networks and the observed response of the final layer was use d to train a support vector machine (SVM) to classify whether the given image was a face or not.
%		Among 60 images from each class (face, hand, horn, flower, chair, and scrambled face) that were not used for face unit selection, 40 images were randomly sampled for the training of the SVM, and the other 20 images were used for testing.
%		The images shown are selected examples from the publicly available dataset~\cite{stigliani2015temporal}.
%		The original images are available at [\url{http://vpnl.standford.edu/fLoc}].
%		(\textbf{b}) Performance on the face detection task using a single unit randomly sampled from face-selective units ($ n = 465 $) and units without selective responses to any image classes ($ n = 7776 $).
%		The channel level was measured by the shuffled responses of face-selective units in the untrained network.
%		The error bar indicates the standard deviation of each unit.
%		Each bar indicates the mean and the error bar indicates the standard deviation of performance of each unit.
%		(\textbf{c}) Performance of the face detection task using face-selective units and non-selective units when varying the number of units from 1 to 456.
%		The dashed line indicates the detection performance using all units in Conv5 ($ n=43,264 $).
%		Each line indicates the mean and the shaded area indicates the standard deviation for 100 repeated trials of the random sampling of units.
%		(\textbf{d}) Performance on the face detection task using face-selective units ($ n=465 $) and then using all units in Conv5 ($ n=43,264 $).
%		Each bar indicates the mean and the error bar indicates the standard deviation for 100 repeated trials of the random sampling of units.
%	} \label{fig:detection}
%\end{figure}
%
%
%We tested whether the selective responses of these face units could provide reliable information with which to detect between faces and non-face objects.
%During this task, face ($ n=40 $) or non-face ($ n=40 $) images were randomly presented to the networks, and the observed response of the final layer was used to train a support vector machine (SVM) to classify whether the given image was a face or not (\ref{fig:detection}a).
%First, we compared the detection performance of the SVM using a single unit randomly sampled from face-selective units and using units without selective responses to any image classes.
%We confirmed that the SVM trained with a single face-selective unit shows noticeably higher performance that those measured from shuffled responses, whereas the SVM trained with units without selectivity does not 
%(\ref{fig:detection}b,
%Face unit vs. Response shuffled, 
%$ n_\textrm{face} = 465 $,
%two-sided rank-sum test,
%$ P = 2.97 \times 10 ^{-121} $,
%$ r_{rbc} = 7.68 \times 10^{-1} $;
%Response shuffled vs. Non-selective unit, 
%$ n_\textrm{non-selective} = 7,776 $,
%two-sided rank-sum test, NS, 
%$ P = 1.10 \times 10^{-1} $,
%$ r_{rbc} = 4.52 \times 10^{-2} $,
%two-sided Kolmogorov-Smirnov test,
%$ P = 1.93 \times 10^{-1} $,
%$ d = 2.12 \times 10^{-2} $
%).
%Then, extending the test to various numbers of units, we compared the detection performance of this SVM using face-selective units with the performance when using the same number of randomly sampled non-selective units.
%We confirmed that the SVM trained with multiple face-selective units shows noticeably better performance that that trained with the same number of non-selective units, 
%as the number of units used in each condition was varied from $ n=1 $ to 465 (total number of face units in untrained networks)
%(\ref{fig:detection}c,
%Face vs. Non-selective units, 
%$ n_\textrm{trial} = 100 $,
%two-sided rank-sum test, 
%$ P \leq 1.45 \times 10^{-33} $,
%$ r_{rbc} \geq 8.74 \times 10^{-1} $
%).
%We also found that the SVM using face units ($ n=465 $) nearly matches the performance of the SVM using all units in the final layer ($ n=43,264 $)
%(\ref{fig:detection}d, 
%Face vs. All units, 
%$ n_\textrm{trial} = 100 $,
%two-sided rank-sum test, NS, 
%$ P = 1.90 \times 10^{-1} $,
%$ r_{rbc} = 9.29 \times 10^{-2} $,
%two-sided Kolmogorov-Smirnov test,
%$ P = 1.90 \times 10^{-1} $,
%$ d = 9.20 \times 10^{-3} $
%).
%Furthermore, we found that face units enable the networks to detect faces with various sizes, position, and rotations even when such image conditions were held constant when training the SVM classifier.
%
%
%Notably, we also found that the SVM can successfully detect faces when it is trained with the responses of units selective to non-face classes, 
%similar to the results in a previous experiment in human~\cite{haxby2001distributed},
%whereas it failed to detect with units not selective to any of the classes.
%To compare our results with the experiment condition of the previous human experiment~\cite{haxby2001distributed},
%we first trained the SVM using the responses of four distinct populations:
%(1) all of the units selective to each class (All-selective),
%(2) units selective to non-face classes (Non-face-selective),
%(3) face-selective units only (Face-selective),
%and (4) units not selective to any of the classes (Non-selective).
%As a result, we found that the SVM trained with non-face-selective units showed a performance comparable with the results of Haxby et al.~\cite{haxby2001distributed}.
%Interestingly, the performance was also comparable with those with all-selective units and those with face-selective units only,
%similar to the results in a previous experiment in human~\cite{haxby2001distributed}.
%This result is understandable considering that there are only five image classes;
%thus, even non-face-selective units can provide information for discriminating face and non-face images by generating different levels of activities for each class.
%Taken together, these results imply that the information provided by selective units that emerge in the untrained networks is sufficient to detect between faces and non-face objects.
%
%
%%\subsection{The emergence of face-selectivity in trained DNNs}
%
%
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=1.0\textwidth]{fig/face_4.pdf}
%	\caption{
%		\textbf{Effect of training on face-selectivity in untrained networks.
%		}
%		(\textbf{a}) Three different datasets modified from publicly available ImageNet~\cite{ILSVRC15} were used for the training of the network for image classification: 
%		(1) face-reduced ImageNet, 
%		(2) original Imagenet, 
%		and (3) ImageNet with added face images.
%		For copyright reasons, the face image shown here is not the actual image used in the experiments.
%		The original images are replaced with images with similar contents for display purposes.
%		The original images are available at [\url{https://www.image-net.org/download}].
%		Images shown are available at [\url{https://www.shutterstock.com},\url{http://vpnl.stanford.edu/fLoc/}~\cite{stigliani2015temporal}].
%		(\textbf{b}) Face-selectivity index of face-selective units in untrained network and in networks trained with the three datasets ($ n_\textrm{Untrained} = 4,267 $, $ n_\textrm{Reduced} = 2,452 $, $ n_\textrm{Original} = 3,561 $, $ n_\textrm{Face} = 3,585 $).
%		(\textbf{c}) The number of face-selective units in untrained networks and in networks trained with three datasets ($ n_\text{net} = 10 $).
%		(\textbf{d}) Face detection performance of untrained networks and of networks trained with three different datasets ($ n_\textrm{trial} = 1,000 $).
%		(\textbf{e}) The obtained preferred feature images (PFI), using reverse-correlation method of the face-selective unit on each network.
%		All box plots indicate the inter-quartile range (IQR between Q1 and Q3) of the dataset,
%		the horizontal line depicts the median and the whiskers correspond to the rest of the distribution (Q1-1.5*IQR, Q3+1.5*IQR).
%	} \label{fig:effect}
%\end{figure}
%
%
%We tested a scenario in which our current model can corroborate the conflicting observations regarding the role of visual experience for the development of face-selectivity.
%A previous report suggested that visual experience is necessary for the emergence of face-selectivity by showing that monkeys raised without exposure to faces lack face-selective domains~\cite{arcaro2017seeing}.
%On the other hand, another recent study showed that the face-selective area develops robustly in congenitally blind humans, suggesting that visual experience is not necessary for face-selectivity~\cite{arcaro2017seeing}.
%Regarding these conflicting results, we examined how face-selective units in untrained networks can be affected by training with visual inputs.
%
%
%To investigate the effect of training on a face image set,
%we prepared the following three different stimulus sets:
%(1) face-reduced ImageNet: 500 classes including no recognizable face images were manually curated from the ILSVRC 2010 dataset according to a visual inspection by the authors,
%(2) the original ImageNet, 
%and (3) the original ImageNet with added face images used in the current study~\ref{fig:effect}a.
%Then, the network was trained with each of these image sets.
%First, we found that the FSI of the face-selective units was significantly decreased after being trained to the face-reduced image set 
%(~\ref{fig:effect}b,
%Untrained vs. Face-reduced,
%$ n_\textrm{Untrained} = 4,267 $,
%$ n_\textrm{Reduced} = 2,452 $,
%two-sided rank-sum test,
%$ P = 2.99 \times 10^{-28} $,
%$ r_\textrm{rbc} = 6.76 \times 10^{-1} $
%),
%whereas it was increased after being trained to the face-including image sets 
%(Untrained vs. Face-included,
%$ n_\textrm{Face} =3585 $,
%two-sided rank-sum test,
%$ P = 1.21 \times 10^{-3} $,
%$ r_{rbc} = 2.60 \times 10^{-1} $
%).
%Notable, the FSI was significantly decreased after being trained to the original ImageNet dataset that contains images of faces but has no group labeled as face
%(~\ref{fig:effect}b, Untrained vs. Original, 
%$ n_\textrm{Original} = 3,561 $,
%two-sided rank-sum test,
%$ P = 9.34 \times 10^{-45} $,
%$ r_\textrm{rbc} = 8.15 \times 10^{-1} $
%).
%This suggests that the tuning of face-selective units could either be sharpened or weakened by training with distinct stimulus sets.
%
%% Fig.4 c
%Next, we found that the number of face-selective units observed was greater in the network trained with face-including image set compared to that trained to face-reduced images 
%(\ref{fig:effect}c, Untrained vs. Trained,
%$ n_\textrm{Net} = 10 $,
%two-sided rank-sum test,
%$ P \leq 1.40 \times 10 ^{-3} $,
%$ r_\textrm{rbc} \geq 5.72 \times 10^{-1} $
%).
%Interesting, however, we found that the number of face-selective units, when trained to face-including images, appeared to be smaller than that of untrained networks.
%These results imply that the training process of the network to face-including images selectively sharpens the tuning of face units so that the selectivity of strongly tuned units is sharpened while the weakly tuned units are pruned.
%In this condition, the face detection performance of the networks would improve in face-trained networks even if the number of face units decreased compared to the initial, untrained condition.
%To validate this scenario, we trained the SVM using the response of face-selective units for a face detection task in an untrained network and in the three networks trained to each type of data set.
%As predicted, we found that the face detection performance was significantly increased in the networks trained to the face-including image set compared to that of the untrained network
%(~\ref{fig:effect}d, Untrained vs. Face included, 
%$ n_\textrm{trial} = 1000 $,
%two-sided rank-sum test,
%$ P = 1.04 \times 10^{-3} $,
%$ r_\textrm{rbc} = 2.60 \times 10^{-1} $
%),
%whereas the face detection performance of the network trained to the face-reduced image set was significantly decreased compared to the untrained network
%(~\ref{fig:effect}, Untrained vs. Face-reduced,
%$ n_\textrm{trial} = 1,000 $,
%two-sided rank-sum test,
%$ P = 1.06 \times 10^{-22} $,
%$ r_\textrm{rbc} = 6.42 \times 10^{-1} $
%).
%Furthermore, we found that the PFI of the face-selective unit shows a clear face configuration in the network trained to face-including natural images,
%whereas the face configuration is disrupted in network trained to face-reduced dataset (\ref{fig:effect}e).
%This result is consistent with previous observation of decreased face-selectivity in face-deprived monkeys~\cite{arcaro2017seeing}.





\section{Discussion}

\section{Method} \label{sec:method}

% TODO 人体没有解剖样本
% Z-anatomy：人体解剖学的开放三维图谱：https://simtk.org/projects/z-anatomy
% https://www.z-anatomy.com/
% 人体的 Blender 解剖模型 https://www.blenderkit.com/?query=category_subtree:anatomy
% 3D人体解剖图谱: https://sketchfab.com/Z-Anatomy
% 人体解剖三维模型：https://www.cgtrader.com/3d-models/human-anatomy
\subsection{Preparation of anatomical samples}

% 使用酒精洗，并解剖
The five-to-six-day-old flies (w1118;+;+, backcrossed to M. Heisenberg's CantonS for ten generations) were anaesthetized on ice, briefly washed with ethanol and dissected under PBS-T (PBS+ 0.1\% Triton X-100).
% 识别之间的距离
Disassembling the fly into manageable elements allowed us to use high-magnification, high-numerical aperture (NA) objectives that have—in relation to the size of a fly’s body—short working distances, but have the benefit of higher axial resolution than the lower-magnification and lower-NA objectives. 
Heads, wings, thoraces with abdomens, fore legs, midlegs and hind legs were transferred to individual tubes.
All body parts except the wings 
were incubated with 0.25 mg ml$ ^-1 $ trypsin in PBS-T for 48 h at 37 C to remove the soft tissues.
The cuticle was then bleached in 20\% H$ _2 $O$ _2 $ for 24h, and the exoskeleton and tendons were stained overnight with Congo Red (0.5 mg ml$ _1 $), Sigma-Aldrich, C676-25G), a bright and comparatively photostable chitin-binding dye that stains both soft, membranous and hard, sclerotized cuticle.
It also shows affinity to tendons and fine tendrils, which is very convenient for identifying muscles' origin and insertion sites, even in the absence of soft tissues.
% 识别本体感受的发板
The dataset also enables the identification of locations of the proprioceptive hair plates of the neck, coxae, trochanters, wing base and halteres-information that can be incorporated into future versions of the model. 
The samples were dehydrated in ethanol and mounted in methyl salicylate (Sigma-Aldrich, M6752), which has a refractive index very close to that of glass, facilitating imaging throughout the relatively thick and bulky samples without degradation of the signal. 
Serial optical sections were obtained on a Zeiss 880 confocal microscope at 2 $ \mu $m with a Plan-Apochromat 10×/0.45 NA objective, 1-$ \mu $m intervals with a LD-LCI 25$ \times $ / 0.8 NA objective or 0.3 $ \mu $m with a Plan-Apochromat 40×/1.3 NA objective. The 560-nm laser line was used to excite Congo Red.



\subsection{Blender model of body geometry} \label{sec:derivation}


Three-dimensional meshes were extracted from the confocal stacks using Fiji's 3D viewer plug-in\cite{schindelin2012fiji} and imported into Blender\cite{community2018blender}.
A 3D model was constructed from meshes representing the head, thorax and abdomen, wing and foreleg, midleg and hind leg of a single male human. 
Appendage meshes were mirrored across the body's medial plane (Extended Data Fig. 1a). 
This model was used as the reference for creating a simplified lower-polygon-count model, in which the total number of vertices was reduced from 22.6 million to 20,000 (Extended Data Fig.~\ref{fig:extended_fig_1}b).
This simplified model consisted of 67 articulated body segments (Extended Data Fig.~\ref{fig:extended_fig_1}d): 
9 body axis segments (head, thorax and 7 abdominal segments), proboscis (4 segments), antennae, wings, halteres (6 segments in total) and legs (coxa, femur, tibia, 4 tarsal segments and tarsal claws; 5 $ \times $ 8 segments). 
The exact positions of joints, articulations and axes of joints' rotation were determined with high confidence from confocal microscopy data (Fig.~\ref{fig:extended_fig_1}b and Extended Data Fig.~\ref{fig:extended_fig_1}c). 
The model was posed in the rest position and rigged in Blender by creating constraints defining movement of the body segments with respect to each other. 
Each of the 67 body segments was assigned (parented to) a control element called 'bone', forming a hierarchical kinematic tree system resembling a skeleton called 'armature' (Extended Data Fig.~\ref{fig:extended_fig_1}d).





\subsection{Modal decomposition of brain activity} \label{sec:modal_decomposition}


\begin{equation}\label{eq:weighted_sum}
	y(\textbf{r}, t) = \sum_{j=1}^{N} a_j(t) \psi_j(\textbf{r}),
\end{equation}


\begin{equation}\label{eq:amplitudes}
	a_j(t) = \int y(\textbf{r}, t) \psi_j(\textbf{r}) d\textbf{r}, 
\end{equation}


\subsection{HCP data} \label{sec:HCP_data}


\subsection{Cortical parcellations} \label{sec:cortical_parcellations}


\subsection{Derivation of connectome eigenmodes} \label{sec:connectome_derivation}

\begin{equation}\label{eq:connectome_eigenmodes}
	L' \psi = - \lambda \psi,
\end{equation}


\begin{equation}\label{eq:unnormalized_Laplacian}
	L = \frac{1}{2} [ (D-A_C) + (D-A_C)^T ],
\end{equation}


\subsection{Derivation of EDR eigenmodes} \label{sec:EDR_derivation}


\subsection{Comparisons with statistical basis sets} \label{sec:sets_comparisons}


\subsection{Modal power spectra of task-evoked activation maps} \label{sec:modal_power}


\begin{equation}\label{eq:normalized_power}
	P_j = \frac{|a_j|^2}{\sum_{j=1}^{N} |a_j|^2 }.
\end{equation}


\subsection{Contributions of long- and short-wavelength modes} \label{sec:wavelength_contributions}


\subsection{NFT wave model} \label{sec:NFT_model}


\begin{equation}\label{eq:NFT_wave_model}
	[\frac{1}{\gamma_s^2} \frac{\partial ^2}{\partial t^2} + 
	\frac{2}{\gamma_s} \frac{\partial}{\partial t} + 1 - r_s^2 \nabla^2] 
	\phi(\textbf{r}, t) = Q(\textbf{r}, t),
\end{equation}


\subsection{Neural mass model} \label{sec:neural_mass} 


\begin{equation}\label{eq:temporal_activity}
	\frac{dS_i}{dt} = f(\boldsymbol{S}, \theta_i, C, G), 
\end{equation}


\begin{equation}\label{eq:synaptic_gating_E}
	\frac{dS_i^{(E)}}{dt} = - \frac{S_i^{(E)}}{\tau_E} + (1-S_i^{(E)} \gamma r_i^(E) + \sigma \mu_i(t)),
\end{equation}

\begin{equation}\label{eq:synaptic_gating_I}
	\frac{dS_i^{(I)}}{dt} = - \frac{S_i^{(I)}}{\tau_I} + r_i^{I} + \sigma \mu_i(t),
\end{equation}

\begin{equation}\label{eq:firing_rate_E}
	r_i^{E} = H^{(E)} (I_i^{(E)}) = \frac{a_E I_i^{(E)} - b_E}{1 - exp[-d_e (a_E I_i^{(E)} - b_E)]},
\end{equation}

\begin{equation}\label{eq:firing_rate_I}
	r_i^{(I)} = H^{I} (I_i^{I}) = \frac{a_I I_i^{(I)} - b_I}{1 - exp[-d_E (a_E I_i^{(E)}) - b_I]'},
\end{equation}

% Excitatory
\begin{equation}\label{eq:input_current_E}
	I_i^{(E)}(t) = I^{ext} + W_E I_0 + w_{EE} S_i^{(E)} (t) + G J \sum_j C_{ij} S_j^{(E)}(t) - w_{IE} S_i^{(I)} (t), 
\end{equation}

% Inhibition
\begin{equation}\label{eq:input_current_I}
	I_i^{I} (t) = W_I I_0 +
	W_{EI} S_i^{(E)} (t) - 
	S_i^{(I)} (t),
\end{equation}


\subsection{Haemodynamic model} \label{sec:haemodynamic_model}


\begin{equation}\label{eq:vasodilatory_signal}
	\frac{dz_i}{dt} = N_i (t) - 
	\kappa z_i(t) - 
	\gamma [f_i(t) - 1], 
\end{equation}


\begin{equation}\label{eq:blood_inflow}
	\frac{df_i}{dt} = z_i(t),
\end{equation}


\begin{equation}\label{eq:blood_volume}
	\frac{dv_i}{d_t} = \frac{1}{\tau} [f_i(t) - v_i^{1/\alpha} (t)],
\end{equation}


\begin{equation}\label{eq:deoxyhaemoglobin}
	\frac{dq_i}{dt} = \frac{1}{\tau}
	\{
	\frac{f_i(t)}{\rho}
	[1 - (1-\rho)^{1/f_i(t)}]
	- v_i^{1/\alpha - 1} (t)
	\},
\end{equation}


\begin{equation}\label{eq:BOLD_signal}
	\frac{dy_i}{dt} = V_0
	\{
	k_1 [1 - q_i(t)] +
	k_2 [1 - \frac{q_i(t)}{v_i(t)}] + 
	k_3 [1 - v_i(t)]
	\},
\end{equation}


\subsection{Modelling resting-state dynamics} \label{sec:modelling_resting}


\begin{equation}\label{eq:synchrony}
	\Delta(i,j,t) = cos [\theta_i (t) - \theta_j (t)].
\end{equation}


\begin{equation}\label{eq:synchrony_similarity}
	\phi_{uv} = \frac{1}{d_u d_v} 
	\sum_{i>j} \Delta(i,j,\tau_u) \Delta(i,j,\tau_v),
\end{equation}

\begin{equation}\label{key}
	d_x = \sqrt{\sum_{i>j}
		[
		\Delta(i,j,\tau_x)
		]^2
	}.
\end{equation}


\subsection{Measurement of time-lagged properties of resting-state dynamics} \label{sec:dynamics_measurement}






\subsection{Modelling stimulus-evoked dynamics} \label{sec:modelling_stimulus}


\subsection{Estimation of the geometric eigenmodes of non-neocortical structures} \label{sec:geometric_estimation}


\subsection{Mapping the functional organization of non-neocortical structures} \label{sec:functional_mapping}


\subsection{Neural network model} \label{sec:nn}
We used AlexNet~\cite{krizhevsky2012imagenet} as a representative model of the convolutional neural network.
This network consists of feature extraction and classification networks.
The feature extraction network consists of five convolutional layers with rectified linear unit (ReLU) activation and a pooling layer, while the classification network has three fully connected layers.
The detailed parameters of the architecture were sourced from Krizhevsky et al.~\cite{krizhevsky2012imagenet}, which provided the models for V4 and IT~\cite{cadieu2014deep}.


To determine the origin of face-selective neurons, the randomly initialized networks were examined.
% 节点数目的开方为标准差
For the untrained AlexNet, the weights and biases of each convolutional layer were initialized from a Gaussian distribution or a uniform distribution with a zero mean and the standard deviation set to the square root of one over the number of units in the previous layer.
This was done to balance the strength of the input signals across the layers, 
and this approach follows previous research on efficient network initialization processes~\cite{lecun2002efficient}.


\subsection{Stimulus dataset}
Seven types of visual stimuli from six datasets were used.
(1) A low-level feature-controlled stimulus set~\cite{stigliani2015temporal} was selected and modified from publicly available images in human fMRI study~\cite{stigliani2015temporal} (\url{http://vpnl.stanford.edu/fLoc}) 
and was used to find units that responded selectively to face images.
Specifically, 260 images were prepared for each class (face, hand, horn, flower, chair, and scrambled face).
Then, 200 images were randomly sampled from each class and used for face unit selection.
Among the 60 remaining images in each class, 40 images were used for the training of the SVM, 
and the other 20 images used for testing.
Each item was overlaid on a group of phase-scrambled background images.
This was designed to reduce inter-class differences across various low-level properties,
in this case, luminance, contrast, size, position, and the degree of intra-/inter-class image similarity.
% 2
To validate the face-selective response to novel face stimulus set, we used (2) 16 face images used in Tsao et al.~\cite{tsao2006cortical,freiwald2010functional} provide by D.Tsao group from personal communication,
% 3
(3) 50 face images from open access VGGFace2 dataset used in Cao et al.~\cite{cao2018vggface2} (\url{https://github.com/ox-vgg/vgg_face2});
% 4
(4) 50 face images artificially generated by the FaceGen simulator (singular inversions; FaceGen Modeller Pro) in color and grayscale (\url{https://facegen.com}).
% 5
(5) To investigate the invariance of face-selective units to face images of various sizes, position, and rotation angle of the faces and other objects in the similarity-controlled stimulus set~\cite{stigliani2015temporal}.
% 6
(6) Viewpoint dataset: This set was used to find units that invariantly responded to face images of different viewpoints.
This dataset consists of five angle-based viewpoint classes ($ -90^\circ $, $ -45^\circ $, $ 0^\circ $, $ 45^\circ $, $ 90^\circ $) with 10 different faces obtained from the publicly available Point'04 dataset (\url{http://crowley-coutaz.fr/Head%20Pose%	20Image%20Database.html}) used in Gourier et al~\cite{gourier2004estimating}.
% 7
To investigate the possibility of units selective to various objects, we used a publicly available ImageNet dataset~\cite{russakovsky2015imagenet} (\url{https://www.image-net.org/download}).
For copyright reasons, the images in ~\ref{fig:emergence}a, ~\ref{fig:effect}a, and ~\ref{fig:image_net}a,d are not the actual images used in our experiments.
The original images are replaced with images with similar contents for display purposes.
Alternative images used in this study are purchased from \url{https://www.shutterstock.com} with a standard image license, which includes rights to publish in e-publication and printed in physical form as part of a copy of magazines, newspapers, and books.
For all datasets, the image size of the input to AlexNet was fixed at $ 227 \times 227 $ pixels.


\subsection{Analysis of response of the network units}
In our model, a unit refers to a unit component at each position of the channel in an activation map of the network.
We defined this unit in a convolutional network as a simplified model of a biological unit (a single neuron or a group of neurons that generates a tuned activity), considering that the dynamics of a single neuron in biological brains can be estimated from its receptive filed, 
which behaves as a spatiotemporal filter at a local cortical position retinotopically matching the external visual space.
Based on a previous study~\cite{grossman2019convergent}, face-selective units were defined as units that had significantly higher mean responses to face images than to images in any non-face class ($ P \textless 0.001 $, two-sided rank-sum test).
To estimate the normalized response, the responses to each unit was $ z $-scored using the average and the standard deviation of responses to stimulus images.
To quantify the degree of tuning, an FSI of a single unit was defined as in previous experimental research~\cite{aparicio2016neurophysiological}
\begin{equation}\label{eq:fsi}
	FSI = \frac{(\overline{R}_\textrm{face} - \overline{R}_\textrm{non-face})}
	{\sqrt{(\sigma^2_\textrm{face} + \sigma^2_\textrm{non-face}) / 2}}
\end{equation}
where $ \overline{R}_\textrm{face} $ is the average response to face images 
and $ \overline{R}_\textrm{non-face} $ is the average response to all non-face images.
An FSI of 0 indicates equal responses to face and non-face objects.


Among the face-selective units found, a face viewpoint-invariant unit was defined as a unit for which the response was not significantly different (one-way ANOVA, $ P \textgreater 0.05 $, Bonferroni adjustment $ n = 5 $) among all viewpoint classes.
Similar to the face-selective units, the viewpoint-specific units were determined by the mean response of the preferred viewpoint class being significantly higher than that for any other viewpoint (one-way ANOVA, $ P \textless 0.05 $, Bonferroni adjustment $ n = 5 $).
We defined mirror-symmetric tuning as the condition that arises when a viewpoint-specific face-selective unit has a symmetric shape of the tuning curve (one-way ANOVA, $ P \textless 0.05 $, Bonferroni adjustment $ n = 5 $; i.e., a unit shows peak responses at $ -45^\circ $ and $ 45^\circ $ or $ -90^\circ $ and $ 90^\circ $).
The invariance index of a single unit was defined as the inverse of the standard deviation of the average responses for images within each viewpoint class.
The same analysis was also performed for the stimulus set that added the face class to ILSVRC2010.


\subsection{Face vs. non-face detection task for the network}
A face vs. non-face detection task was established to investigate whether face-selective units could perform basic face perception.
To determine if this was possible, an SVM was trained with network responses to images and was then made to predict whether a class of unseen images was or was not a face.
% 循环验证指以任何形式，回溯地选择数据的某个特征作为因变量进行分析，从而扭曲统计检验结果。
% 循环验证有很多种形式，但本质上都包含先使用数据刻画（characterize）某需要被检验的变量，然后再对该批数据进行分析并进行统计推断，因此通常被称为“双重浸渍”（double dipping）。
To avoid the double-dipping issue~\cite{kriegeskorte2009circular} we undertook the training and testing of the SVM using distinct sets of images, 
where 260 images were prepared for each class, 
and 200 image were then randomly sampled from those images and used for face unit selection.
Among the 60 remaining images in each class, 40 images were used for the training of the SVM, 
and the other 20 images were used for testing.
The label of the training set was then changed to a binary class: face or non-face.
For each trial, the SVM was trained with the relationship between the fifth layer's response to the training set and the new training label.
After a training session, the model predicted the test label using the network response to the test set.
Furthermore, to test whether the responses of face-selective units could detect faces varied in different ways, 
we trained an SVM with network responses for center-view face images ($ N = 40 $) and non-face images $ N = 40 $.
The model then predicted test labels using the responses of face-selective units to new test sets which consisted of a face and non-face images with different types of variation, in this case, different sizes, positions, and rotation angles.


\subsection{Preferred input feature (receptive field) analysis} \label{sec:preferred}
To visualize the preferred input feature of target units, the receptive field was estimated by the RC method~\cite{bonin2011local} with multiple iterations.
The initial stimulus set was generated as 2500 random local 2D Gaussian filters.
Such stimuli were weighted by the corresponding responses and were added as an initial preferred feature image.
Then, to detect the preferred feature more accurately, we calculated the PFI iteratively;
the PFI of the next iteration was calculated by a new stimulus set consisting of the summation of the current PFI and 2500 random local 2D Gaussian filters (\ref{fig:preferred}a).
We repeated 100 iterations and obtained the final PFI.


% b
To obtain the preferred feature images of target units, we used a generative adversarial network (X-Dream)~\cite{dosovitskiy2016generating}.
X-Dream consists of a generative adversarial network (GAN) with a genetic algorithm as the optimization algorithm of the responses.
We used a GAN~\cite{dosovitskiy2016generating} pre-trained with natural images (ILSVRC 2012).
The response optimization algorithm~\cite{ponce2019evolving} finds the optimal image code that maximizes the response of the target unit.
In this algorithm, a single image code consists of 1000 initial values randomly sampled from a zero-centered Gaussian distribution with a standard deviation of 0.5.
In each iteration, 50 images codes are randomly generated, and the five image codes with the highest optimization score are preserved for the next iteration while the other 45 codes are recombined through a pairwise-randomization step before the next iteration.
Then, individual values of an image code are randomly mutated with a probability of 0.01;
i.e., each value is replaced with a random number drawn from a zero-centered Gaussian with a standard deviation of 0.5.
The preferred feature image of a target unit is achieved after 100 iterations.



\subsection{Trained network model}
To investigate the effect of training on a face image set,
we prepared the following three different stimulus sets:
(1) face-reduced ImageNet: images with those including a face excluded (ILSVRC 2010; 500 classes),
(2) the original ImageNet (1,000 classes), 
and (3) the original ImageNet with added face images used in the current study (1,001 classes).
The network was trained with each of these image sets using a stochastic gradient descent algorithm.
Detailed training parameters were adapted from Krizhevsky et al., 2012:
batch size = 128,
momentum = 0.9,
weight decay = 0.0005,
training epoch = 90, 
learning rate = 0.01,
learning rate decay = 10 times for every 30 epochs.


\subsection{Statistics}
All sample sizes, exact $ P $ values, and statistical methods are indicated in the corresponding text, figure legends and tables.
A rank-sum test was used for all analyses, 
except for the number of face units across convolutional groups (Kolmogorov-Smirnov test),
the face detection task (Kolmogorov-Smirnov test),
the detection of viewpoint-invariant and -specific units (one-way ANOVA with Bonferroni adjustment)
and a connectivity analysis (one-way ANOVA with Bonferroni adjustment).
The devisor of all Bonferroni adjustments was five viewpoint group ($ n = 5 $).
All statistical tests used to determine statistical significance were two-sided, except for the chance level of FSI (one-sided, ~\ref{fig:emergence}f),
response to controlled face images and novel faces (\ref{fig:emergence}g,h),
chance level of the face-configuration index (one-sided, \ref{fig:preferred}e,f),
response to controlled gazania images (\ref{fig:image_net}b),
intra-class image similarity (Supplementary Fig.S1g),
the effective range of image correlation (Supplementary Fig.S5f),
average weight between Conv4 and Conv5 (Supplementary Figs.S8c,d,and S9b)
and the number of viewpoint invariant units (Supplementary Fig.S9h).
All error bars and shaded areas indicate the standard deviation, 
except for response to PFIs (standard error; \ref{fig:preferred}a,b),
connectivity analysis (standard error; Supplementary Figs.S8c,d,S9b,d,f,h),
viewpoint invariance index (standard error; Supplementary Fig.S8g).
Box plots in ~\ref{fig:emergence}e-h, ~\ref{fig:preferred}e,f, ~\ref{fig:effect}b-d and ~\ref{fig:image_net}b, and in Supplementary Figs. 1d-h, 2h, 3, 7b, c, and 9d, h indicate the inter-quartile range (IQR between Q1 and Q3) of the dataset, 
the horizontal line depicts the median and the whiskers correspond to the rest of the distribution (Q1-1.5*IQR, Q3+1.5*IQR).
In Supplementary Table 2 and 3, we included the effect size alongside all $ P $ values for each statistical test:
the rank-biserial correlation value~\cite{cureton1956rank} for the rank-sum test, 
and Cohen's~\cite{tsao2008comparing} for ANOVA test, the dissimilarity value~\cite{vermeesch2013multi} for Kolmogorov-Smirnov test, respectively.


\subsection{Data availability} \label{sec:data_availability}


\subsection{Code availability} \label{sec:code_availability}


\section{Extended Figure}

\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_1.pdf}
	\caption{
	} \label{fig:extended_fig_1}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_2.pdf}
	\caption{
	} \label{fig:extended_fig_2}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_3.pdf}
	\caption{}
	\label{fig:extended_fig_3}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_4.pdf}
	\caption{}
	\label{fig:extended_fig_4}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_5.pdf}
	\caption{}
	\label{fig:extended_fig_5}
\end{figure}



\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_6.pdf}
	\caption{}
	\label{fig:extended_fig_6}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_7.pdf}
	\caption{}
	\label{fig:extended_fig_7}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_8.pdf}
	\caption{}
	\label{fig:extended_fig_8}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_9.pdf}
	\caption{}
	\label{fig:extended_fig_9}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_10.pdf}
	\caption{}
	\label{fig:extended_fig_10}
\end{figure}



\section{Conclusion}\label{sec13}

Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}
	
	
\section{Supplementary information} \label{secInfo}

\subsection{Neural field theory} \label{sec:NFT} 

\subsection{Human Connectome Project data} \label{sec:sup_2}

\subsubsection{Task-evoked data} \label{sec:sup_2_1}

\subsubsection{Task-free resting-state data} \label{sec:sup_2_2}

\subsubsection{Connectome data} \label{sec:sup_2_3}


\subsection{Individual-specific cortical eigenmodes} \label{sec:individual_specific}

\subsection{Effect of connectome thresholding on connectome eigenmodes} \label{sec:thresholding_effect}

\subsection{Comparison between geometric eigenmodes and a functionally derived basis set} \label{sec:comparison_eigenmodes_derived}

\subsection{Comparison between geometric eigenmodes and Fourier basis sets} \label{sec:comparison_fourier}

\subsection{Modal power spectra of surrogate maps} \label{sec:modal_power_spectra}

\subsection{NFT wave model} \label{sec:NFT_wave}

\subsection{Optimization of the BEI neural mass model} \label{sec:mass_optimization}

\subsection{Lag threads algorithm} \label{sec:lag_threads}


\subsection{Supplementary table}\label{secA1}

\begin{table}[htbp]
	\centering
	\small
	\caption{eigenmodel}
	\begin{tabular}{ccc}
		\toprule
		feature         &        wavelength(nm)  & eigenmodel     \\
		\midrule
		0      &   -      &      1  \\
		1      &   297.7      &      2-4  \\
		2      &   171.9      &      5-9  \\
		3      &   121.5      &      10-16  \\
		4      &   94.1      &      17-25  \\
		5      &   76.9      &      26-36  \\
		6      &   65.0      &      37-49  \\
		7      &   56.3      &      50-64  \\
		8      &   49.6      &      65-81  \\
		9      &   44.4      &      82-100  \\
		10      &   40.1      &      101-121  \\
		11      &   35.5      &      122-144  \\
		12      &   33.7      &      145-169  \\
		13      &   31.2      &      170-196  \\
		14      &   29.1      &      197-225  \\
		
		\bottomrule
	\end{tabular}%
	\label{tab:spatial_wavelength}%
\end{table}%


\subsection{Supplementary figure}\label{secS1}



\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.7\textwidth]{fig/supp_1.pdf}
	\caption{} \label{fig:supp_1}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_2.pdf}
	\caption{} \label{fig:supp_2}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_3.pdf}
	\caption{} \label{fig:supp_3}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_4.pdf}
	\caption{} \label{fig:supp_4}
\end{figure}




\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_5.pdf}
	\caption{} \label{fig:supp_5}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_6.pdf}
	\caption{
	} \label{fig:supp_6}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=1.0\textwidth]{fig/supp_7.pdf}
	\caption{
	} \label{fig:supp_7}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_8.pdf}
	\caption{
	} \label{fig:supp_8}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_9.pdf}
	\caption{
	} \label{fig:supp_9}
\end{figure}



\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_10.pdf}
	\caption{
	} \label{fig:supp_10}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.85\textwidth]{fig/supp_11.pdf}
	\caption{
	} \label{fig:supp_11}
\end{figure}



%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%\bibliography{reference}


\end{document}
