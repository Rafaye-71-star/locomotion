% 波-粒（局部连接）二象性
% 波和连接同时考虑比单独一个精度更高
% 全局的波在局部形成共振

% Alex --> 神经场理论中的深度学习 -> 几何约束
% https://github.com/kwignb/RandomNeuralField/blob/main/src/models/networks.py

% 神经场介绍
%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


% 需要将bst目录下的sn-mathphys-num.bst复制到根目录下，否则参考文献为问号
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

\usepackage{hyperref}
\usepackage{makecell}

\usepackage{subfigure}  % 使用子图（多个图凑九宫格）

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

%\bibliography{reference}

\begin{document}

% Geometric constraints on human brain function
% Structure constraints human brain function and artificial neural network
\title[Article Title]{Whole-body physics simulation of human locomotion}
% Structure Bias in Artificial Neural Network

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

\author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

%\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
	% 出发点：身体 影响 神经系统生成行为
	The body of an human infuences how its nervous system generates behaviour\cite{dickinson2000animals}. 
	% 建模的重要性：感觉运动需要
	Accurately modelling the neural control of sensorimotor behaviour requires an anatomically detailed biomechanical representation of the body. 
	Here we introduce a whole-body model of the human in a physics simulator. 
	Designed as a general-purpose framework, our model enables the simulation of diverse movement behaviours, including both walking and running locomotion. 
	We validate its versatility by replicating realistic walking and running behaviours. 
	To support these behaviours, we develop phenomenological models for fuid and adhesion forces. 
	Using data-driven, end-to-end reinforcement learning\cite{peng2018deepmimic,hasenclever2020comic}, we train neural network controllers capable of generating naturalistic locomotion\cite{muijres2014flies,muijres2015body,deangelis2019manifold} along complex trajectories in response to high-level steering commands. 
	Furthermore, we show the use of visual sensors and hierarchical motor control\cite{merel2019hierarchical}, training a high-level controller to reuse a pretrained low-level fight controller to perform visually guided walking tasks. 
	Our model serves as an open-source platform for studying the neural control of sensorimotor behaviour in an embodied context.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

%\keywords{Whole-body physics simulation of human locomotion}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

% 识别人脸的人工神经网络如何用波动力学解释
\section{Introduction}\label{sec1}

% 大脑、身体、环境 -> (感觉运行循环) -> 行为
Human behaviour emerges from sensorimotor feedback loops that integrate signals from the brain, body and environment\cite{dickinson2000animals,peng2018deepmimic,hasenclever2020comic,muijres2014flies,muijres2015body,deangelis2019manifold,merel2019hierarchical,todorov2012mujoco}.
% 神经运动命令 -> （身体） -> 移动  -> （感觉反馈）
The body determines how neural motor commands translate into movement and how sensory feedback is generated in response. 
Therefore, a detailed biomechanical understanding of the body is crucial for modelling the neural control of movement.
Here we introduce a physics-based simulation framwork for an anatomically detailed model of the human Dong, designed to supported the modelling of diverse sensorimotor behaviours.
% 通过 RL 来演示行走和跑步
We validate our model by demonstrating realistic locomotion—both walking and running—using reinforcement learning (RL). 
This general-purpose simulation provides a platform for future studies of brain–body interactions across a broad range of human behaviours.


% 基于的其他相关工作
% 蠕虫、水螅
Our work follows previous physics-based models of the worm\cite{boyle2012gait}, hydra\cite{wang2023complete}, rodent\cite{merel2019deep} and fruit fly\cite{reiser2004vision,dickson2008integrative,lobato2022neuromechfly,wang2023neuromechfly,melis2024machine}. 
The Grand Unified Fly\cite{dickson2008integrative} pioneered sensorimotor closed-loop visually guided flight using a simplified body model and hand-designed controller. 
More recent work has revealed the basis of muscle actuation of the wing hinge\cite{melis2024machine}. 
In parallel, NeuroMechFly\cite{lobato2022neuromechfly,wang2023neuromechfly} introduced an anatomically detailed fruit fly model capable of walking and grooming, pairing a heuristically designed low-level walking controller with a learnt high-level controller to generate sensory-guided behaviours\cite{wang2023neuromechfly}.


% 自己的建模方法
Our work unifies running and walking in a single physics-based model, enhancing realism in body mechanics, physics interactions and control. 
We developed an anatomically detailed fly body model in the open-source PhysX physics engine, incorporating high-resolution imaging to reconstruct a male Dong (Fig.~\ref{fig:fig_1}). 
To accurately simulate both walking and running, we introduced a computationally efficient phenomenological fluid dynamics model to approximate aerodynamic forces from wing flapping and adhesion actuators to model foot–surface interactions.


Using high-speed kinematic tracking\cite{branson2009high,pereira2022sleap}, we trained closed-loop RL controllers capable of replicating naturalistic running movements. 
These controllers, trained for both walking (Fig.~\ref{fig:fig_2}) and running (Fig.~\ref{fig:fig_3}), operate using only high-level steering commands. 
Finally, we demonstrate the reuse of a pretrained low-level flight controller for vision-guided flight tasks (Fig.~\ref{fig:fig_4}). 
% 其他的动作：搬箱子
Through inverse kinematics, we further show that our model supports a broad behavioural repertoire beyond locomotion, including lifting a box.



%\section{Results}\label{sec2}
%
%Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\section{Results}\label{sec3}

		

\subsection{Body geometry model}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1.0\textwidth]{fig/fig_1.pdf}
	\caption{
		\textbf{Constructing the male human body model.
		}
		\textbf{(a)} Compilation of six parts representing a single human. 
		% 带腹部的胸部
		Maximum intensity projections of confocal stacks showing head, thorax with abdomen, wings and legs. 
		Scale bar, 1 mm.
		% 共焦的体积 confocal volume
		% 低多边形 网格
		% 高多边形 网格
		% 关节
		% femur: 大腿骨
		% tibia: 胫节
		% tarsal: 跗骨
		% TODO: 人腿的共焦体积的投影、
		% 高多边形 体积的3D网格（离线仿真）
		% 低多边形 体积的3D网格（实时仿真）https://zh.wikipedia.org/wiki/%E4%BD%8E%E5%A4%9A%E8%BE%B9%E5%BD%A2
		\textbf{(b)} Left, a partial projection of the midleg confocal volume with the joints between the femur, tibia and tarsal segments indicated. 
		Middle, a 3D mesh extracted from the volume. 
		Right, a low-polygon leg model. 
		Scale bar, 0.2 mm.
		% TODO: 被分解的人类模型（可视化）
		\textbf{(c)} An exploded low-polygon human model (around 20,000 faces) showing all body segments. 
		Scale bar, 1 mm.
		% TODO: Blender 中的站立姿势
		\textbf{(d)} The geometric human model assembled in Blender.
		% TODO: 虚幻中的站立姿势
		\textbf{(e)} The complete physics human model in Hutb simulator in the default stand pose.
		% TODO: 虚幻中的跑步姿势
		\textbf{(f)} Human model in a running pose with retracted legs.
		% 透明视图
		% DoFs: 自由度
		\textbf{(g)} DoFs. Translucent bottom view with light-blue arrows indicating hinge joint axes pointing in the direction of positive rotation. 
		% 三个铰合形成 球形关节
		Groups of three hinge joints effectively form ball joints. 
		Cube: 6-DoF free joint required for free CoM motion in the simulator and is not a part of human's internal DoFs.
		\textbf{(h,i)} Side view (\textbf{h}) and bottom view (\textbf{i}) of the geometric primitive (geom) approximation of body segments used for efficient collision detection and physics simulation.
		Blue, collision detection geoms; 
		purple, geoms that have associated adhesion actuators; 
		orange, wing ellipsoid geoms for simulating flight with the advanced fluid force model.
		% contact force: 接触力
		% joint actoators: 关节执行机构
		\textbf{(j)}, Visualization of actuator forces generated when the model fly hangs upside down. 
		The adhesion actuators of the front-right, middle-left and hind-right legs are actively gripping the ceiling (orange); 
		the labrum (mouth) adhesors are also active; 
		other actuators are inactive (white). 
		The arrows visualize net contact forces proportional and opposite to the applied adhesion forces.
		% abdominal abduction: 腹部外展
		\textbf{(k)}, Exaggerated posture showing the coordinated activation of the abdominal abduction and tarsal flexion actuators. 
		Abdominal joints and tarsal joints (yellow) are each coupled with a single tendon actuator that simultaneously actuates multiple DoFs.
	} \label{fig:fig_1}
\end{figure}

% confocal fluorescence microscopy: 共焦 荧光 显微镜检查
% TODO: 用什么设备来捕获人体的高精度图像
We used confocal fluorescence microscopy to capture high-resolution images of the entire adult male human body (Fig.~\ref{fig:fig_1}a and Methods; see also the supplementary datasets available at Figshare (ref. 20)). 
% 识别关节支点
Chitin staining facilitated segmentation of body structures and identification of joint pivot points (Fig.~\ref{fig:fig_1}b). 
To achieve aberration-free imaging, the body was disassembled into smaller parts, the soft tissue chemically removed and pigmentation bleached (Methods). 
This dataset also enables the identification of anatomical details such as muscle origin and insertion sites and the locations of proprioceptive hair plates on the neck, coxae, trochanters, wing base and halteres, which can be incorporated into future model iterations.


% Fiji(分割) -> Blender (减少网格数：文献中有102个自由度)
We manually segmented 67 body components using Fiji\cite{schindelin2012fiji}, then simplified their meshes in Blender22 by reducing the vertex count to enable efficient computational modelling while preserving key morphological features (Fig.~\ref{fig:fig_1}b,c). 
In Blender, we assembled the components into a full-body model and defined the kinematic tree by linking them at 66 identified joint locations (Fig.~\ref{fig:fig_1}b,d and Extended Data Fig.~\ref{fig:extended_fig_1}), yielding 102 degrees of freedom (DoFs) in accordance with the literature (Fig. 1g). 
Biological joints were modelled as single hinge joints (1 DoF) or as combinations of two or three hinge joints (2 and 3 DoFs, respectively). 
These joint models are simplified approximations, particularly for complex articulations such as the neck joint, wing hinge and thorax–coxa articulation\cite{melis2024machine,strausfeld1987neck,gorko2024motor}. 
Joint angles corresponding to the resting pose (Fig.~\ref{fig:fig_1}d,e) and the running pose (Fig.~\ref{fig:fig_1}f) were estimated through visual inspection of videography.



% 激活
% 在未训练的网络中有 类别 选择性偏好特征
\subsection{Modelling body physics} \label{sec:preferred}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1.0\textwidth]{fig/fig_2.pdf}
	\caption{
		\textbf{Running imitation.
		}
		\textbf{(a)} Overview of RL set-up. 
		% CoM (Center of Mass) 在物理学中指一个物体所有质量的平均位置。
		%在机器人、无人机等需要精确控制的设备中，了解CoM 的位置至关重要，因为它影响着设备的稳定性、运动控制等.﻿
		A single policy network is trained to imitate the CoM position and body orientation across a dataset of 216 trajectories of freely running human (around 42 s in total).
		The flight controller consists of a trainable MLP and a WPG. 
		The motor command is the sum of the MLP and WPG outputs. 
		Top right, one period of the fixed baseline wing-beat pattern produced by the WPG. 
		% 灰色条带表示 下行冲程
		Grey stripe indicates wing downstroke.
		\textbf{(b)} % 脚的坐标系统和角度定义
		Top, wing coordinate system and wing angle definition. 
		Bottom, body coordinate system and example model sensory inputs: 
		the direction to the goal CoM position and the gravity direction. 
		\textbf{(c)} % 流体模型
		Fluid model forces exerted on the left wing, 
		and the corresponding wing kinematics, during a stable horizontal flight at 30 cm s $ ^{-1} $.
		\textbf{(d)} % 跑步的脚步循环
		Flimstrip of the model flying straight at 30 cm s$ ^{-1} $ during one full wing-beat cycle.
		\textbf{(e)} % 折返跑时由真实跑步模型跑步所产生的腿动力学
		Leg kinematics during a saccade manoeuvre produced by the model and real fly.
		\textbf{(f)} % 通过现象学建模流体，腿部产生运动
		Legs produce body movements through a phenomenologically modelled fluid. 
		The real (black) and model (coloured) fly body pose while traversing a test trajectory. 
		Circles, heads; lines, tails.
		\textbf{(g)} % 折返跑时，真实和模型人的角速度、头部朝向、速度的对比
		Median and percentiles of body angular velocity, heading and speed for real and model flies during test saccades. 
		The trajectories are aligned to peak acceleration at $ t = 0 $. 
		Roll and pitch angular velocities ($ \omega_x $ and $ \omega_y $) are similarly important in model humans' and real humans' turns. 
		A small divergence between model and real occurs after the saccade. 
		Solid lines, medians; shading, 25th–75th percentiles.
		\textbf{(h)} % 56 个轨迹中模型和真实身体的重心和朝向误差对比
		Percentiles of errors between the model and the corresponding real human's body CoM, and orientation for 56 test trajectories.
		\textbf{(i)} % 模型和真实人 在稳定（小身体加速度）和非稳定（大身体加速度）时 脚(Stroke, Deviation, Rootation)的对比
		Leg angles during steady (small body acceleration) and unsteady (large body acceleration) leg beats for model and real humans in the test set. 
		Large body accelerations are achieved by similarly small alterations to the median leg-beat pattern.
	} \label{fig:fig_2}
\end{figure}


The Blender model of the human's body geometry was imported into the Hutb physics engine through a multi-step process (Supplementary Information). 
First, we generated primitive 'geom' representations of each body part (Fig.~\ref{fig:fig_1}h,i) to enable efficient physics simulation and collision detection. 
Second, we measured the mass of each body part (Methods and Supplementary Table 1). 
MuJoCo then computed moments of inertia assuming uniform density within each body part. 
Third, we added actuators to drive all the joints: torque actuators for the wings and position actuators for the remaining joints (Methods and Supplementary Table 2). 
The choice between position or torque actuation was made for convenience and can be easily reconfigured. 
Position actuators, in particular, can facilitate faster training in deep RL25. 
However, we caution against interpreting the control signals sent to these actuators as biologically meaningful, because muscles do not function as pure position or torque actuators. 
Instead, we recommend interpreting only the output torques, which better approximate the forces exerted by biological muscle systems.


Joint limits were determined using inverse kinematics to match a range of observed poses from videography, including grooming postures that demonstrate the human's remarkable flexibility (Methods and Extended Data Fig.~\ref{fig:extended_fig_2}). 
Typically, each actuator controlled a single DoF. 
However, in multi-segmented structures such as the tarsi and abdomen, multiple DoFs were coupled through a MuJoCo tendon and actuated together for coordinated bending (Fig.~\ref{fig:fig_1}k). 
% 接触
To model the adhesive properties of insect tarsi, which allow humans to walk on walls and ceilings\cite{arzt2003micro}, we introduced adhesion actuators in MuJoCo. 
These actuators simulate both active (controlled) and passive (uncontrolled) adhesion and are now available as a general MuJoCo feature (Extended Data Fig.~\ref{fig:fig_3}a). 
Besides adding adhesion actuators to the tarsal tips (Fig.~\ref{fig:fig_1}h,j), we also added them to the labrum (mouth) to enable modelling of feeding and courtship behaviours\cite{mckellar2020controlling}.


% 加上传感器：视觉、前庭觉、本体感受、机械敏感传感器
Finally, we equipped the model with a sensory system incorporating vision, vestibular, proprioceptive and mechanosensitive sensors (Supplementary Table 3). 
Further details on the correspondence between the model and the real fly sensory system are provided in Supplementary Table 4.


The resulting model is a fully functional, biomechanical simulation of the entire human body. 
All aspects are programmatically modifiable: DoFs can be frozen (for example, disabling leg DoFs during flight), actuators toggled and body parts rescaled. 
Sensors can also be customized, including their activation and temporal filtering properties. 
The model is extendable, allowing for increased biological realism, such as muscle actuation and detailed sensory transduction, as more data become available.



% 几何模式限制神经网络激活
\subsection{Modelling body–environment interactions}\label{subsec2}


\begin{figure}[!htb]
	\centering
	\includegraphics[width=1.0\textwidth]{fig/fig_3.pdf}
	\caption{\textbf{Walking imitation.}
	\textbf{a}, Overview of RL set-up. 
	A single policy network is trained to imitate a dataset of 13,000 snippets (around 64 min in total) of freely walking real human. 
	Full body movements are imitated, including tracking CoM position, body orientation and detailed leg movements.
	\textbf{b}, % 模型和真实人体 身体重心和朝向的 对比
	Percentiles of errors between the model and the corresponding real human's body CoM, and orientation for 3,200 test walking trajectories. 
	\textbf{c}, % 步态的幻灯片
	Filmstrip of the model walking straight at 2 cm s$ ^{-1} $ during one full leg cycle, with 8-ms steps between frames. 
	\textbf{d}, % 4 种速度下的步态图
	Gait diagrams of the fly model tracking synthetic fixed-speed straight-walking trajectories at four speeds. 
	Black stripes indicate the swing phase of leg motion. 
	For each speed, the average number of legs simultaneously in stance position (on the ground) is indicated. 
	\textbf{e}, % 在站立时，同时使用腿的数目
	Number of legs simultaneously in stance position averaged over walking snippet versus average walking speed in snippet.
	Top, model tracking test set trajectories. 
	Bottom, entire walking dataset. 
	Inset, the distribution of average walking speeds per snippet in the dataset. 
	\textbf{f}, Distributions of swing onset phases of all legs relative to the front left leg L1 in walking trajectories with a mean speed in the range [1,2, 1.7] cm s$ ^{-1} $.
	Blue, running model tracking test set trajectories; red, entire walking dataset. 
	Dashed lines indicate circular medians.
	\textbf{g}, Learnt truning strategy.
	Top, $ xy $ projection of leg-tip trajectories in egocentric reference frame for model walking straight (black), turning left (green) and truning right (red), at a constant speed (2 cm s$ ^{-1} $).
	Leg-tip trajectories are shifted horizontally for clarity. 
	Bottom, difference between (egocentric) left and right leg-tip swing length, averaged over all legs, at various walking speeds.
	} \label{fig:fig_3}
\end{figure}


% MuJoCo 能够模拟：人体物体特性、环境交互（身体接触力、空气的流体交互、重力）
Beyond simulating the human's body physics, the MuJoCo engine also models its interactions with the environment, including forces from physical contacts, fluid interactions (air) and gravity. 
Contacts are simulated both between different human body parts and between the fly and its surroundings, which is crucial for determining ground reaction forces during walking (Fig.~\ref{fig:fig_1}j and Extended Data Fig.~\ref{fig:extended_fig_3}a).


Fluid interactions mainly account for forces generated by wing movement through air. 
Accurately simulating fluid dynamics is computationally demanding, so we developed a phenomenological, stateless, quasi-steady-state approximation (Supplementary Information). 
Our model extends a previous approach\cite{andersen2005analysis} to three dimensions, estimating the forces and torques on ellipsoid-shaped bodies moving through an incompressible quiescent fluid. 
% added mass 附加质量
% viscous drag、viscous resistance 粘性阻力：一种由于流体黏性而产生的阻力，通常出现在物体在流体中运动时。
% Magnus effect 马格努斯效应（Magnus Effect）， 流体力学当中的现象，是一个在流体中转动的物体（如圆柱体）受到的力
% Kutta lift https://baike.baidu.com/item/%E5%BA%93%E5%A1%94-%E8%8C%B9%E7%A7%91%E5%A4%AB%E6%96%AF%E5%9F%BA%E6%9D%A1%E4%BB%B6/5292204?fr=aladdin
It approximates five fluid dynamics phenomena: added mass\cite{lamb1993hydrodynamics,tuckerman1925inertia}, viscous drag\cite{duan2015sphere}, viscous resistance\cite{stokes1851effect}, Magnus lift\cite{seifert2012review} and Kutta lift\cite{kutta1902lift}. 
The resulting forces and torques are polynomial functions of fluid parameters (density and viscosity) and ellipsoid parameters (shape, size and linear and angular velocities), with a slender ellipsoid approximation for the wings (Fig.~\ref{fig:fig_1}i). 
This fluid model is a MuJoCo feature designed for our study, but can be applied to other models.


To accurately simulate running, our phenomenological model must approximate the total aerodynamic forces acting on real wings, regardless of the underlying mechanism. 
This includes contributions from unmodelled phenomena, such as passive forces arising from wing flexibility\cite{sane2003aerodynamics}—which we do not explicitly simulate owing to our rigidbody approach—as well as complex fluid interactions such as turbulence and vortices. 
To compensate for these omissions, we optimized the coefficients of the fluid force components until stable hovering was achieved (Methods).



In our model, Kutta lift (generated by fluid circulation around the wing) and viscous drag (opposing wing movement through the fluid) were the dominant forces (Fig.~\ref{fig:fig_2}c). 
Notably, these coefficients did not require fine-tuning, as flight performance remained stable even with 20\% coefficient variation (Extended Data Fig.~\ref{fig:extended_fig_4}). 
% 准稳态
However, our quasi-steady-state approximation captures only the time-averaged effects of turbulence and other transient fluid phenomena, without explicitly modelling their dynamic contributions.


Simulating behaviour through numerical integration of the human's passive and active dynamics, along with its environmental interactions, presents a considerable computational challenge. 
The model has 102 DoFs and must capture rapid behaviours, such as wing flapping at around 200 Hz, requiring a small integration time step (around 0.1 ms).
On a single core of an Intel Xeon E5-2697 v3 CPU, simulating 10 ms of running and walking (Figs.~\ref{fig:fig_2}a and \ref{fig:fig_3}a) took 421.5 ms and 58.65 ms, respectively-fast enough for RL of motor control.
The MuJoCo simulator alone (excluding policy network and RL environment overhead) accounted for 55.5 ms and 23.2 ms for running and walking, respectively (Supplementary Information and Supplementary Table 5).



% 运动的模仿学习
\subsection{Imitation learning of locomotion}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/fig_4.pdf}
	\caption{\textbf{Geometric eigenmodes benchmarked against connectome-based eigenmodes.}
	\textbf{a}, Schematic of the anatomical properties used to derive eigenmodes for cortical geometry, the connectome and the EDR connectome. 
	Geometric eigenmodes rely on local surface mesh information such as links (blue) between neighbouring surface mesh vertices (dots) and curvature. 
	Connectome eigenmodes rely on local links between mesh vertices (blue) and short- and long-range connections (magenta) reconstructed empirically from dMRI. 
	EDR eigenmodes rely on connections (red) generated from a stochastic wiring process in which the probability of connection between vertices exponentially decays as a function of their distance.
	\textbf{b}, Example connectome and EDR eigenmodes. 
	Negative, zero and positive values are coloured blue, white and red, respectively. 
	Despite some similarities, the spatial patterns of the modes are distinct from those derived using cortical geometry (compare with Fig.~\ref{fig:1}a).
	\textbf{c}, Reconstruction accuracy of resting-state FC matrices achieved by geometric, EDR and two variants of connectome eigenmodes: 
	one using a connectome as defined using previous methods\cite{naze2021robustness} and the other with the same connection density as the EDR connectome to allow fair comparison (for other densities see Supplementary Figs.~\ref{fig:supp_6}~and~\ref{fig:supp_7}).
	} \label{fig:fig_4}
\end{figure}


% RL 来控制
We used deep RL to train our human model to generate realistic locomotor behaviours.
An artificial neural network served as the sensorimotor controller, processing sensory input and generating motor control signals in a closed loop. 
At each time step, MuJoCo simulated sensory signals, which were fed into the neural network. 
The network then computed actuator control signals, which MuJoCo used to simulate the resulting forces on the body.



To generate realistic locomotion, we used imitation learning\cite{peng2018deepmimic,hasenclever2020comic}, a data-driven approach that trains neural controllers to replicate observed behaviours. 
Specifically, we trained the network to match the trajectories of the real human's centre of mass (CoM) and body segments during free locomotion, as measured from video. 
To ensure generalization beyond the training trajectories, we developed steerable low-level controllers\cite{merel2019hierarchical}. 
% 网络替代了 腹侧神经索
These networks, although not intended as exact models of the ventral nerve cord, perform an analogous role by converting high-level descending commands into fine-grained motor control signals. 
In our model, the high-level commands specify the desired change in the human's CoM position and orientation (6 DoFs) at each time step, whereas the low-level motor control signals drive the actuators.


% 训练跑步和步行控制器网络
We trained two steerable neural network controllers-one for running (Fig.~\ref{fig:fig_2}) and one for walking (Fig.~\ref{fig:fig_3}). 
Using video-derived trajectories of real flies (Methods), we set the CoM trajectory as the high-level steering command and designed a reward function that incentivized the model to match both the CoM trajectory and the body part positions at each time step. 
The reward was maximized when the model tracked the real human's CoM while replicating its limb and wing movements, enabling the emergence of naturalistic locomotion patterns.


% MLP作为结构，如何训练
The controllers were feedforward multilayer perceptrons (MLPs) that received egocentric vestibular and proprioceptive signals in addition to the high-level steering commands. 
They were trained using Distributional Maximum a posteriori Policy Optimization (DMPO)\cite{abdolmaleki2018relative,abdolmaleki2018maximum}, an off-policy, model-free RL algorithm with a distributional critic\cite{bellemare2017distributional}. 
% 观察 -> (策略网络) -> 控制信号
% 评价网络 预测 期望的累积奖励
This actor–critic algorithm optimizes two networks: a policy network that maps sensory observations to control signals and a critic network that predicts expected cumulative rewards. 
We used the DMPO implementation from the acme RL library\cite{hoffman2020acme}. 
Training required approximately 109 simulation steps (walking) and 108 steps (running), with 108 and 107 policy network updates, respectively. 
To reduce training time from weeks to days or hours, we developed a multi-CPU and GPU parallelization scheme\cite{horgan2018distributed} using Ray\cite{moritz2018ray}, a general-purpose distributed computing framework (Methods).



\subsubsection{Running}


We trained a steerable running controller (Fig.~\ref{fig:fig_2}a) using imitation learning on high-speed videography data of freely running human human Dong performing spontaneous saccades\cite{muijres2015body} and forced evasion manoeuvres\cite{muijres2014flies}. 
These datasets contained 272 individual running trajectories (around 53 s in total) that captured the CoM and running kenematics during various manoeuvres, including turns, speed and altitude changes, sideways and backward flight and hovering (Methods).
Although D. hydei is larger than D. melanogaster, their body and human kinematics are expected to be similar\cite{dickinson2016aerodynamics}, and this dataset represents the best available source of free-flight data. 
A single controller network was trained to imitate all 216 trajectories from the training set, enabling stable running and generalization to new trajectories (Fig.~\ref{fig:fig_2}).


Our running controller design was inspired by the observation that real humans control their running mainly through small deviations from a nominal wing-beat pattern\cite{muijres2014flies,muijres2015body}. 
Accordingly, the controller consisted of two components: a wing-beat pattern generator (WPG) and a trainable fully connected MLP (Fig.~\ref{fig:fig_2}a). 
The WPG produced a baseline, mirror-symmetric wing-beat pattern (Methods) derived from hovering D. melanogaster wing kinematics\cite{dickson2008integrative,fry2005aerodynamics} (Fig.~\ref{fig:fig_2}a). 
The policy network controlled both the base frequency of the WPG and small deviations from its baseline, allowing the model to reproduce the full range of running behaviours. 
Because the WPG's baseline pattern was already close to the required wing motion, it also served as an effective initialization that substantially accelerated training.


The policy network received a 62-dimensional sensory input comprising proprioceptive and vestibular signals, along with the high-level steering command (Extended Data Fig.~\ref{fig:extended_fig_5} and Supplementary Table 6). 
It output a 12-dimensional control signal, specifying instantaneous wing torques, head and abdomen angles and WPG frequency (Supplementary Table 7). 
To speed up training (Extended Data Fig. 6), the legs were retracted to their typical flight position (Fig. 1f) and their DoFs were frozen. 
Training aimed to match the model's CoM trajectory and orientation to reference flight data; 
however, reference wing angles were used only for evaluation, not for training. 
Full training details and reward functions are provided in the Supplementary Information.


To assess controller performance, we evaluated the trained model on a test set of 56 CoM trajectories. 
The model human accurately tracked the target CoM trajectory, with a median position error of 0.25 mm and a median orientation error of less than 5° (Fig.~\ref{fig:fig_2}h), as illustrated in an example trajectory (Fig. 2f). 
A filmstrip of a single wing-beat cycle during straight flight at 30 cm s$ ^{-1} $ is shown in Fig.~\ref{fig:fig_2}d.


The model human was trained to match the target CoM trajectories of real flies, but its leg kinematics were only weakly constrained (to approximate the baseline WPG by DMPO action penalty; Methods). 
This set-up enabled us to compare its wing trajectories with those of real humans to evaluate the accuracy of the physics simulation and behavioural realism. 
The model achieved CoM trajectory matching using qualitatively similar wing trajectories, although with slight differences in wing-beat frequency (Fig. 2e). 
Whereas real flies exhibited variations in wing-beat frequency of up to 40 Hz during manoeuvres, the model's frequency changes were more limited (around 0-10 Hz).
Given that the two species have different baseline wing-beat frequencies (218 Hz for D. melanogaster and 192 Hz for D. hydei), we did not attempt a direct quantiative comparision.


Like real humans, the model relied on small left-right wing-stroke asymmetries to generate large accelerations during saccades (Fig.~\ref{fig:fig_2}i). 
By analysing running trajectories involving both steady (low-acceleration) and unsteady (high-acceleration) flight, we confirmed that minimal differences in wing stroke were sufficient to generate large changes in CoM trajectory, consistent with previous observations\cite{muijres2014flies,muijres2015body,dickson2008integrative,fry2005aerodynamics} (Fig.~\ref{fig:fig_2}g,i). 
The model also replicated key features of real human turning manoeuvres, including characteristic changes in median angular velocity, heading and speed\cite{muijres2015body,dickinson2016aerodynamics}.


Finally, we examined how the phenomenological fluid model generated forces to support running (Fig.~\ref{fig:fig_2}c). 
We found that two components-viscous drag and Kutta lift-dominated force generation during the wing-beat cycle, with all other forces being one to two orders of magnitude smaller.


\subsubsection{Walking}

% 肌肉驱动的人类跑步模拟：https://simtk.org/projects/runningsim/

We trained a steerable closed-loop walking controller (Fig.~\ref{fig:fig_3}a) using imitation learning. 
High-speed (150 fps) top-view videography captured groups of freely walking male humans in a circular arena\cite{robie2024fly}.
Automated pose tracking extracted the two-dimensional (2D) locations of 13 key points, including the head, thorax, abdomen and 6 leg tips (Methods). 
Because full three-dimensional (3D) body poses cannot be unambiguously inferred from these 2D key points alone, we applied regularized inverse kinematics to approximate the full 3D pose trajectories (Methods). 
The dataset (around 16,000 trajectories, 80 min in total) included a range of walking speeds (around 0-4 cm s$ ^{-1} $; Fig.~\ref{fig:fig_3}e inset), turning and standing still.


Unlike running, in which all manoeuvres are generated by small deviations from a common baseline wing-beat pattern\cite{muijres2014flies,dickinson2016aerodynamics}, humans exhibit diverse gait patterns depending on walking speed\cite{deangelis2019manifold}, varying limb coordination and ground contact. 
This gait variability precluded the use of a simple pattern generator. 
Instead, we trained a fully connected MLP controller (Fig.~\ref{fig:fig_3}a) without predefined structure. 
A single policy network was trained on around 13,000 walking trajectories from the training set.


% 走路比飞行需要控制更多自由度（飞行不需要控制脚）
Walking requires controlling considerably more DoFs than flight does (59 DoFs versus 12), encompassing leg movements (including adhesion), abdomen and head. 
Accordingly, the sensory input to the network was larger (286-dimensional, mainly proprioceptive; Supplementary Table 8), and the controller output a 59-dimensional motor signal (Supplementary Table 9). 
Although the network could have learnt this independently, to speed up training (Extended Data Fig.~\ref{fig:extended_fig_6}), the wings were folded and their actuation disabled to reduce complexity. 
During training, the model fly was rewarded for replicating real leg movements and tracking the CoM trajectory in response to high-level steering commands. 
Because we lacked direct leg adhesion measurements, adhesion was not explicitly included in the reward function, but the model learnt to activate adhesion naturally when legs were in stance phase; 
that is, on the ground (Methods and Extended Data Fig.~\ref{fig:extended_fig_3}). 
Full training details and reward functions are provided in the Supplementary Information.



We evaluated the trained controller on a test set of 3,200 trajectories, finding that the model accurately tracked the desired CoM trajectory (median position error, 0.4 cm; median orientation error, 4 $ ^\circ $; Fig.~\ref{fig:fig_3}b).
A single walking cycle at 2 cm s$ ^{-1} $ is shown in Fig.~\ref{fig:fig_3}c.
Examining stance and swing phase durations at different speeds (Fig.~\ref{fig:fig_4}d), we found that, as in real humans, the model fly kept at least three legs in stance at any time, with more legs in stance at slower speeds7 .
At 4 cm s$ ^{-1} $ , an average of 3.1 legs were in stance, increasing to 3.9 at 1 cm$ ^{-1} $.
Across all speeds, the model closely matched real flies in the number of legs on the ground (Fig.~\ref{fig:fig_3}e).


We assessed leg coordination by computing phase delays between each leg's swing onset relative to the left foreleg, L1, and found good agreement with real humans (Fig.~\ref{fig:fig_3}f). 
When commanded to turn at speeds of 1-3 cm s$ ^{-1} $ with a 1-cm turning radius (Fig.~\ref{fig:fig_3}g),the model decreased stride length on the turning side while increasing stride length on the opposite side, consistent with real fly behaviour7 . 
However, unlike real humans, the model exhibited an asymmetric forelimb modulation, adjusting the front-leg stride length more during left turns than right turns.


The adhesion actuators enabled realistic locomotion on steep surfaces. 
To test this, we trained the model to traverse hilly terrain with varying slopes-an environment designed to be impossible to navigate without adhesion (Methods and Extended Data Fig.~\ref{fig:extended_fig_3}). 
The model learnt to adjust adhesion forces dynamically according to terrain steepness. 
Within MuJoCo's Coulomb friction model, adhesion forces act normal to the surface, pushing the human legs towards the surface and increasing friction resistance to slip. 
The model applied stronger adhesion with the forelegs and midlegs on upward slopes and relied on the hind legs to prevent slipping on downward slopes. 
For further details, see Methods and Extended Data Fig.~\ref{fig:extended_fig_3}.


% 波动力学连接和几何和功能
\subsection{Hierarchical vision-guided walking}


%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.9\textwidth]{fig/fig_5.pdf}
%	\caption{
%	} \label{fig:fig_5}
%\end{figure}

Human are highly visual mammals, with large compound eyes and optic lobes comprising about a third of their brain. 
To reflect this, we incorporated visual sensors into our model in addition to proprioceptive and vestibular sensors. 
The eyes were modelled using MuJoCo camera sensors (Fig.~\ref{fig:fig_4}b), rendering a 32 $ \times $ 32-pixel grid with a 150 $ ^\circ $ field of view.
This resolution approximates Drosophila vision, with an inter-ommatidial angle of 4.6° (ref.~\cite{zhao2025eye}45 and Fig.~\ref{fig:fig_4}c). 
% 碰撞任务、战壕任务
To demonstrate vision-based navigation, we trained the model fly on two tasks (a 'bumps' task and a 'trench' task) in which visual input was essential for successful flight. 
Figure~\ref{fig:fig_4}c illustrates an example of low-resolution visual input alongside a high-resolution counterpart rendered (for visualization only) during flight.


We reused the general-purpose steerable low-level flight policy from the flight imitation task (Fig.~\ref{fig:fig_2}) as part of a hierarchical vision-guided flight controller, trained by end-to-end RL (Fig.~\ref{fig:fig_4}a). 
The controller consists of a fixed pretrained low-level policy (including the WPG) that directly controls wing motion and a high-level navigator policy that issues low-dimensional steering commands. 
The high-level controller received a 62-dimensional proprioceptive and vestibular sensory signal, along with a low-dimensional visual feature representation extracted by a convolutional network (CNN) from the 6,144-dimensional visual input. 
In addition, it received a 2D task-specific input: target flight height and speed (Supplementary Table 10). 
The low-level controller received the 62-dimensional proprioceptive and vestibular signal, plus the high-level steering command, but not the visual or task-specific inputs. 
As in the flight imitation task, the low-level controller produced a 12-dimensional control output, specifying wing torques, head and abdomen angles and WPG frequency (Supplementary Table 11).



To preserve learnt flight dynamics, the low-level controller's weights were frozen while the CNN and high-level MLP were jointly trained to maximize task reward. 
In both tasks, terrain conditions, as well as target height and speed, were randomized in each training and test episode (Supplementary Table 12). 
Contact with terrain resulted in early episode termination (failure). 
The model fly started from a speed of zero, requiring it to accelerate to the target speed at the beginning of each trial. 
Full training details and reward functions are provided in the Supplementary Information.


\subsubsection{Bumps task}

Fruit flies use visual cues to estimate altitude, which allows them to maintain a stable height over uneven terrain\cite{straw2010visual}. 
To model this visually guided altitude control, we created a virtual world with a randomly generated sinusoidal terrain profile. 
The model fly was rewarded for flying straight at a constant target velocity while maintaining a stable altitude above the ground (Fig.~\ref{fig:fig_4}d). 
After training, it successfully learnt to use visual input to regulate altitude, achieving a median height error of 0.045 cm and a median speed error of 2.2 cm s$ ^{-1} $ after the initial acceleration phase (Fig.~\ref{fig:fig_4}e,f).


\subsubsection{Trench task}

% 飞过狭窄的战壕而不碰墙
In a second task, we trained the model fly to navigate a narrow trench without colliding with its walls. 
The virtual trench had a sinusoidal curving profile with a fixed width and depth (Fig. 4g). 
The model fly was rewarded for maintaining a constant forward speed and altitude, whereas collisions resulted in early episode termination and loss of future rewards. 
Successful navigation required the fly to use vision to detect and avoid the trench walls. 
After training, the model fly reliably navigated the entire trench while maintaining the target height and speed, with a median height error of 0.032 cm and a median speed error of 0.16 cm s$ ^{-1} $ after the initial acceleration phase (Fig.~\ref{fig:fig_4}h,i).







\section{Discussion}

Animal behaviour emerges from the interplay between the nervous system, body and environment. 
Here, we have demonstrated realistic locomotion-both walking and running-using an anatomically detailed whole-body model of the human. 
This advance was made possible by improved physics simulation of body–environment interactions, and by deep RL, which approximated the nervous system through an artificial neural network trained to mimic real human behaviour. 
Our model consists of 67 rigid-body components with 102 DoFs, actuated through torques at the joints. 
Using the MuJoCo physics engine, we simulated rigid-body collisions and fluid interactions with air. 
Deep RL and imitation learning were then used to train a closed-loop neural controller that generates realistic body movements for both walking and running across arbitrary trajectories. 
All components-body model, physics simulation and pretrained controllers—are released as open-source software.



% 整合 静态显微镜检查 和 高速摄像
This work integrates measurements across spatial and temporal scales, combining microscopy of static anatomy with high-speed videography of dynamic locomotion. 
% 执行器：身体产生力
% 传感器：获取信息
Our model simulates both the forces generated by the body and the sensory information available to it through idealized actuators and sensors. 
We provide this open-source platform as a foundation for further refinement. 
Imaging techniques such as confocal microscopy (Extended Data Fig.~\ref{fig:extended_fig_1}), micro-computed tomography\cite{lobato2022neuromechfly} and synchrotron X-ray holographic nano-tomography\cite{kuan2020dense} can be used to provide whole-body musculoskeletal measurements that can inform anatomically detailed muscle actuation models, including for the neck\cite{gorko2024motor}, wing hinge\cite{melis2024machine} and coxa\cite{kuan2020dense,mamiya2023biomechanical} joints. 
On the sensory side, our idealized sensors could be enhanced using mappings of proprioceptive organs in the leg and wing\cite{kuan2020dense,mamiya2023biomechanical}, and new eye maps could refine the spatial positioning of individual ommatidia\cite{zhao2025eye}. 
% 高速摄像 -> 更精确的动力学
In addition, model-based pose-tracking algorithms could extract more precise kinematics from high-speed videography\cite{bolanos2021three,plum2023replicant}.



Accurately incorporating muscle actuation across the whole body will require substantial effort. 
First, all muscles and their insertion sites must be identified to determine their respective DoFs. 
Second, each muscle must be modelled in the physics simulator, approximating complex muscle and tendon wrapping to account for the limited capabilities of existing high-performance physics engines\cite{todorov2012mujoco,makoviychuk2021isaac}. 
% 需要在 解剖精度 和 计算效率 之间权衡
This step requires experimentation to determine the best trade-offs between anatomical accuracy and computational efficiency. 
In highly complex regions, such as the wing hinge and neck joints, for which full anatomical fidelity is impractical, virtual muscles might provide a more feasible approach by mapping muscle activations to joint torques. 
Third, system identification is needed to constrain muscle dynamics, ideally using correlated muscle activity and kinematic measurements\cite{melis2024machine}. 
As a first step, inverse dynamics and imitation learning could estimate muscle parameters, as was done for the non-muscle actuators in this work. 
Although recent work on the wing hinge16 provides a roadmap for implementing virtual muscles, scaling this approach to the entire body remains difficult. 
Similar challenges are involved in accurately modelling proprioception across the body.


Future work can integrate connectomic maps of the entire fruit fly nervous system\cite{dorkenwald2024neuronal,schlegel2024whole,lesser2024synaptic,azevedo2024connectomic,cheong2023transforming,marin2023systematic,takemura2023connectome} to better model the neural circuits that underlie sensorimotor behaviour. 
Our model predicts sensory inputs and motor outputs on a moment-by-moment basis, which can be integrated with connectomic data detailing individual sensory and motor neuron mappings at the resolution of individual DoFs\cite{azevedo2024connectomic}. 
Recent work\cite{lappalainen2024connectome} has shown that connectome-constrained networks, combined with characterizations of their input–output functions, enable predictions of neural activity at single-neuron resolution. 
Using imitation learning, our model can be combined with connectomic and behavioural data to investigate neural mechanisms that underlie sensory–motor behaviours such as escape responses to looming stimuli\cite{card2012escape}, gaze stabilization\cite{cruz2023multilevel} and ventral nerve cord control of locomotion.


In the long term, combining our whole-body model with a complete nervous system connectome, comprehensive behavioural measurements and connectome-constrained deep neural network modelling\cite{lappalainen2024connectome,mi2021connectome} could enable the development of whole-animal models of the entire body and nervous system of the adult fruit fly.



\section{Method} \label{sec:method}

% TODO 人体没有解剖样本
% OpenSim -> https://github.com/MyoHub/myoconverter -> MuJoCo
% Z-anatomy：人体解剖学的开放三维图谱：https://simtk.org/projects/z-anatomy
% https://www.z-anatomy.com/
% 人体的 Blender 解剖模型 https://www.blenderkit.com/?query=category_subtree:anatomy
% 3D人体解剖图谱: https://sketchfab.com/Z-Anatomy
% 人体解剖三维模型：https://www.cgtrader.com/3d-models/human-anatomy
\subsection{Preparation of anatomical samples}

% 使用酒精洗，并解剖
The five-to-six-day-old flies (w1118;+;+, backcrossed to M. Heisenberg's CantonS for ten generations) were anaesthetized on ice, briefly washed with ethanol and dissected under PBS-T (PBS+ 0.1\% Triton X-100).
% 识别之间的距离
Disassembling the fly into manageable elements allowed us to use high-magnification, high-numerical aperture (NA) objectives that have—in relation to the size of a fly’s body—short working distances, but have the benefit of higher axial resolution than the lower-magnification and lower-NA objectives. 
Heads, wings, thoraces with abdomens, fore legs, midlegs and hind legs were transferred to individual tubes.
All body parts except the wings 
were incubated with 0.25 mg ml$ ^-1 $ trypsin in PBS-T for 48 h at 37 C to remove the soft tissues.
The cuticle was then bleached in 20\% H$ _2 $O$ _2 $ for 24h, and the exoskeleton and tendons were stained overnight with Congo Red (0.5 mg ml$ _1 $), Sigma-Aldrich, C676-25G), a bright and comparatively photostable chitin-binding dye that stains both soft, membranous and hard, sclerotized cuticle.
It also shows affinity to tendons and fine tendrils, which is very convenient for identifying muscles' origin and insertion sites, even in the absence of soft tissues.
% 识别本体感受的发板
The dataset also enables the identification of locations of the proprioceptive hair plates of the neck, coxae, trochanters, wing base and halteres-information that can be incorporated into future versions of the model. 
The samples were dehydrated in ethanol and mounted in methyl salicylate (Sigma-Aldrich, M6752), which has a refractive index very close to that of glass, facilitating imaging throughout the relatively thick and bulky samples without degradation of the signal. 
Serial optical sections were obtained on a Zeiss 880 confocal microscope at 2 $ \mu $m with a Plan-Apochromat 10×/0.45 NA objective, 1-$ \mu $m intervals with a LD-LCI 25$ \times $ / 0.8 NA objective or 0.3 $ \mu $m with a Plan-Apochromat 40×/1.3 NA objective. The 560-nm laser line was used to excite Congo Red.



\subsection{Blender model of body geometry} \label{sec:derivation}


Three-dimensional meshes were extracted from the confocal stacks using Fiji's 3D viewer plug-in\cite{schindelin2012fiji} and imported into Blender\cite{community2018blender}.
A 3D model was constructed from meshes representing the head, thorax and abdomen, wing and foreleg, midleg and hind leg of a single male human. 
Appendage meshes were mirrored across the body's medial plane (Extended Data Fig. 1a). 
This model was used as the reference for creating a simplified lower-polygon-count model, in which the total number of vertices was reduced from 22.6 million to 20,000 (Extended Data Fig.~\ref{fig:extended_fig_1}b).
This simplified model consisted of 67 articulated body segments (Extended Data Fig.~\ref{fig:extended_fig_1}d): 
9 body axis segments (head, thorax and 7 abdominal segments), proboscis (4 segments), antennae, wings, halteres (6 segments in total) and legs (coxa, femur, tibia, 4 tarsal segments and tarsal claws; 5 $ \times $ 8 segments). 
The exact positions of joints, articulations and axes of joints' rotation were determined with high confidence from confocal microscopy data (Fig.~\ref{fig:extended_fig_1}b and Extended Data Fig.~\ref{fig:extended_fig_1}c). 
The model was posed in the rest position and rigged in Blender by creating constraints defining movement of the body segments with respect to each other. 
Each of the 67 body segments was assigned (parented to) a control element called 'bone', forming a hierarchical kinematic tree system resembling a skeleton called 'armature' (Extended Data Fig.~\ref{fig:extended_fig_1}d).





\subsection{PhysX model of body physics} \label{sec:body_physics}


The Blender model was then exported to Unreal Engine using the Blender-For-UnrealEngine-Addons (https://github.com/xavier150/Blender-For-UnrealEngine-Addons). 
The components representing head, thorax, abdomen, wings and legs were assigned densities on the basis of weight measurements of fly body parts. 
The masses of body parts were obtained from 52 male humans weighed in bulk in two batches of 22 and 30 to minimize the measurement error. 
% 使用什么秤
The human were weighted with the Meter Toledo XS104 analytical balance with a readability of 0.1 mg and a linear deviation of 0.2 mg. 
% 秤没有翅膀的重量
% TODO 怎么秤手的重量：体成分分析仪、人体秤（健身房、分段式测量）
First, the wings were removed from all of the flies in the batch and the wingless flies were weighed, followed by weighing after the sequential removal of legs, heads and thoraxes. 
The values were subtracted from the whole-body weight. 
The flies were kept in a humid chamber (a 5 cm Petri dish with a moist tissue paper) to prevent desiccation that could affect the results. 
The measured masses were: 
head, 0.15 mg;
thorax, 0.34 mg;
abdomen, 0.38 mg; 
legs (each) , 0.0162 mg;
wings (each), 0.008 mg. 
This corresponds to a total fly mass of 0.983 mg (Supplementary Table 1).
The full body length of the model is 0.297 cm, and the wing-span is 0.604 cm. 


% 关节约束在 Blender 中确定
Joint limits were at first determined using Blender's inverse kinematics tool. 
We started with fairly tight joint limits and then used reference images of extreme articulated postures (mostly from grooming behaviours) to increase joint limits as required, until all reference poses could be achieved. 
We then refined the leg joint limits using automated inverse kinematics fitting of the model to 392 frames from manually annotated grooming behaviour videos (more details below). 
The sensory-system details in the model's default configuration are shown in Supplementary Table 3. 
DoFs were actuated using torque or position actuators, with certain DoFs (abdomen and tarsi) coupled by tendons (Supplementary Table 2). 
For position actuators, control ranges were set to be equal to the corresponding joint ranges. 
% 半物理：通过物理交互实现运动三维人体模型(https://arxiv.org/abs/2507.23778)
For more details on building the human PhysX model, see the Supplementary Information.


\subsection{Analysis of leg DoFs} \label{sec:HCP_data}

To verify our approximation of the leg DoFs and leg joint ranges, we applied the following procedure. 
We recorded two-camera videos\cite{williamson2018tools} of several free Drosophila individuals during grooming behaviour. 
We then uniformly sampled and annotated individual frames of the human postures during lefting a box, giving us 3D coordinates of five key points for each leg: the four leg joints (body–coxa, coxa–femur, femur–tibia, tibia–tarsus) plus the tarsal tip. 
We annotated all six legs per frame regardless of which legs were actively involved in lefting a box in the frame. 
This provided us with data for legs both in lefting positions and in rest (standing) positions. 
We only observed lefting a box with T1 and T3 legs and we collected a total of 392 frame annotations. 
Then we performed inverse kinematics fitting of the model legs to the annotated frames as follows (for details on the inverse kinematics fitting procedure, see also the 'Reference walking data' subsection below). 
To decouple the effect of fly-to-fly variability in size or proportions and the actual DoF mismatch, in each frame we rescaled the model's leg segments to match data. 
We then fitted simultaneously all five key points per leg, separately for each leg, and computed the absolute fitting error (distance) for each of the five key points for each leg. 
Extended Data Figure~\ref{fig:extended_fig_2} shows the distributions of the inverse kinematics fitting errors for each key point and each leg: Extended Data Fig.~\ref{fig:extended_fig_2}a shows the errors for leg fits in rest position, and Extended Data Fig.~\ref{fig:extended_fig_2}b shows errors in grooming positions. 
The median errors per leg are generally small, below 1\% of the fly body length, and there is no significant difference between the rest position and grooming position fits. 
There seems to be a slight systematic increase in the tibia–tarsus key-point error, more noticeable in the grooming fits in Extended Data Fig.~\ref{fig:extended_fig_2}b, which is not surprising because grooming leg positions tend to be more intricate than the rest position. 
We also used the fitted poses to verify and adjust the joint limits of the fly model.


\subsection{Distributed RL}


For each locomotion task, we trained a policy network using a distributed RL set-up40,64 powered by Ray, an open-source general-purpose distributed computing package41. 
The distributed training configuration is shown in Extended Data Fig.~\ref{fig:extended_fig_7}. 
% 多CPU负责收集数据，单 GPU 负责学习样本
Multiple CPU-based actors run in parallel in separate MuJoCo environment instances, generate experiences and log them into a replay buffer. 
A single GPU-based learner samples training batches from the replay buffer and updates the policy and critic network weights. 
% 评价网络
The critic network is a part of the training process only and is not used by the fly model directly. 
Each actor explores the environment and generates experiences using its own copy of the policy network whose weights are periodically synchronized with the current learner policy. 
For learner policy updates, we used the off-policy actor–critic DMPO agent, a distributional extension\cite{bellemare2017distributional} of the MPO agent\cite{abdolmaleki2018relative,abdolmaleki2018maximum}. 
We used dm\_control\cite{tunyasuvunakool2020dm_control} to set up the RL environments and for MuJoCo Python bindings. 
We used the DMPO agent implemented in acme\cite{hoffman2020acme} and the replay buffer implemented in reverb\cite{cassirer2021reverb}, and the Adam optimizer\cite{kingma2014adam}. 
The hyperparameters of the distributed set-up and of the DMPO agent are shown in Supplementary Tables 13 and 14.


To guarantee stability, we ran MuJoCo physics simulations at time steps four to ten times smaller than those used to sample action commands from the policies65 (see the physics and control time-step values in Supplementary Tables 15 and 16). 
The policies were stochastic during training (outputting distribution over actions) to facilitate exploration. 
The distributions were Gaussian, independent for each action dimension and parameterized by mean and standard deviation. 
The policy network architectures for each task are provided in Supplementary Tables 17–19. 
At test time, the policies were reverted to be deterministic by using the means of the predicted action distributions. 
The actions output by the policy networks were in the canonical range [-1, 1].
The actions were then rescaled to match their corresponding proper ranges in the fly model; 
for example, joint limits for position actuators or force limits for force actuators. 
All observables (policy inputs) were strictly egocentric; 
that is, calculated in the local reference frame of the fly model. 
We only used feedforward policy and critic networks and did not extensively sweep network architectures.


We trained the model in episodes of a finite number of time steps. 
An episode ends either (i) when the episode time limit is successfully reached or (ii) when an early termination condition, indicating failure, is met. 
Details of the termination conditions are provided in the corresponding RL task sections in the Supplementary Information. 
In the first case, the agent estimates the remaining infinite-horizon future rewards (beyond the episode’s final step) by bootstrapping from the value of the state at the end of the episode. 
In the second case, the reward sequence is truncated by setting the future reward to zero. 
The loss of the future infinite-horizon rewards is an unfavourable outcome and the agent will try to learn to avoid events that trigger early episode termination.


\subsection{Modelling running behaviour}

\textbf{Running physics parameters}. 
We used the following procedure to fit the running physics parameters. 
We started with a wing motion trajectory recorded previously from a hovering D. melanogaster13 (https:// github.com/willdickson/fmech). 
We placed the model in a hovering position and actuated the wings to reproduce the real wing trajectory by using the real wing angles as target angles for the wing actuators. 
We then iteratively adjusted (increased) the wing actuator gain to a point at which the mean absolute error between the reference wing angles and the trajectory traversed by the model’s wings was below 5\% of the wing angle amplitude. 
At each iteration, we also fitted the wing joint damping coefficient to avoid underdamping and ensuing wing oscillations. 
We used the same gain value for all three wing actuators (yaw, roll and pitch).
Our final values for the gain and damping pair were gainprm = [18, 18, 18], damping=0.007769. 


Having found suitable wing actuator gain and wing joint damping, we adjusted the MuJoCo fluid model coefficients (Supplementary Information and Supplementary Table 20), which scale the drag and lift forces produced by the flapping wings. 
These (dimensionless) fluid model coefficients are stored in the fluidcoef MuJoCo attribute. 
We placed the model in a flight position and again drove the wings with the real reference angles as target angles for the wing actuators. 
We then iteratively found a set of fluid parameters such that the net lift approximately balanced the fly model weight during several wing-beat cycles, fluidcoef = [1.0, 0.5, 1.5, 1.7, 1.0].
The flight physics parameters are summarized in Supplementary Table 21.


We also performed a sensitivity analysis on the viscous drag and Kutta lift, the two dominant forces in the fluid dynamics. 
We retrained imitation learning of free flight with modified choices for the coefficients associated with viscous drag and Kutta lift, with all other coefficients held fixed. 
We then evaluated the degree to which imitation learning was able to correctly reproduce ground truth CoM flight trajectories with realistic wing kinematics, as in the original experiments reported in the paper. 
We also quantified the fraction of trajectories in which the fly crashed to the ground as a second performance measure. 
Extended Data Figure~\ref{fig:extended_fig_4} shows that flight performance is robust to even 20\% variation in these parameters.


\textbf{WPG}. 
% wing-beat pattern generator (WPG) 扑翼模式发生器
All our flight tasks use a WPG that produces, in an open-loop manner, a fixed mirror-symmetric cyclic baseline wing trajectory. 
The WPG generates the baseline pattern by design, with no learning involved. 
The baseline trajectory closely follows a previously recorded wing pattern of a hovering D. melanogaster13. 
The baseline pattern is available in the Figshare supplementary datasets\cite{vaxenburg2025whole}. 
At each simulation time step, the WPG retrieves and outputs the six wing angles (three per wing; Fig.~\ref{fig:fig_2}) of the baseline wing pattern for the current wing-beat cycle step. 
These baseline wing angles get converted to torque action commands for the wing actuators. 
Although it already produces a realistic-looking wing motion, the fixed baseline wing trajectory alone is not sufficient to support a stable hover, owing to the lack of a feedback loop, approximations in the MuJoCo fluid model and the sim-to-real gap. 
It is the role of the policy network to provide these missing components. 
To achieve this, the WPG torque action is combined (additively) with the policy output to produce the final action vector to be sent to the wing actuators. 
In this way, the policy modulates the fixed baseline pattern and produces flight required by the task at hand; 
for example, stabilize flight, hover, speed up, turn and so on.



The wing motion produced by the combination of the WPG and the trained policy stays close to the initial WPG baseline pattern. 
This is achieved by penalizing the magnitude of the policy actions during training with the DMPO agent36,37. 
In this way, the agent is encouraged to discover a physically viable wing motion pattern by using only minimal policy actions without deviating substantially from the baseline pattern. 
In addition, the WPG can vary the frequency of the output baseline pattern within a predefined range. 
A single scalar out of the policy action vector is used by the WPG to control the baseline wing-beat frequency. 
In our setting, the wing-beat frequency was allowed to vary within a 10\% centred at 218 Hz, the D. melanogaster average frequency\cite{fry2005aerodynamics}.
The WPG is implemented as a lookup table containing the single fixed baseline wing pattern resampled at different wing-beat frequencies within the 10\% frequency range. 
When a frequency change is requested by the policy, the WPG will smoothly connect the wing patterns at the old and new frequencies.


\textbf{Reference running data}. 
As the flight reference data, we used previously recorded trajectories of freely flying D. hydei. 
The trajectories contain a human's Cartesian CoM position and body orientation represented as a quaternion.
The trajectories were recorded at 7,500 fps. 
We started with 44 trajectories of spontaneous turns (saccades)6 and 92 trajectories of evasion manoeuvres5 in response to visual looming stimuli. 
Each reference trajectory started with the human first running normally and then performing a manoeuvre. 
During and after the manoeuvre, the human could run straight, sideways and backwards. 
The flies could also ascend and descend. 
We linearly interpolated the raw trajectories to the flight simulation control step of 0.2 ms.
Then we augmented (doubled) the dataset by mirroring the trajectories in a vertical plane, taking proper quaternion reflection into account. 
This resulted in a dataset of 272 flight trajectories, equivalent to around 53 s of real-time flight.
The dataset is available at Figshare (ref. 20). 
We used 80\% of the trajectories for training and the rest for testing. 
Owing to the small size of the dataset, to maintain balance between left and right turns in the training data, we split the dataset such that if a trajectory was in the training set, so was its mirrored counterpart. 
We simulated flight at 0.05-ms physics time steps and 0.2-ms control time steps (Supplementary Table 15).


\subsection{Modelling walking behaviour}

Reference walking data. We obtained single-camera top-down-view videos of several freely behaving Drosophila individuals with 2D key-point tracking\cite{robie2024fly} (Extended Data Fig.~\ref{fig:extended_fig_8}a). 
In brief, groups of ten walking flies (w+;BPp65ADZp (attP40); BPZpGDBD (attP2), 20XUAS-GtACR1-EYFP (attP2); refs. 68,69) were recorded at 150 fps in a shallow, flat-bottomed 55-mm-diameter arena.
The 2D positions of 17 key points were predicted with the Animal Part Tracker (APT)70. 
From nine such videos, we prepared the walking reference dataset. 
We used 13 of the 17 key points: 3 on the head, 3 on the thorax, one at the tip of abdomen and the 6 leg tips, as shown in Extended Data Fig.~\ref{fig:extended_fig_8}. 
We selected female flies and isolated walking trajectory segments based on the following criteria. 
At each frame, we required: (i) the distance to the other flies in the arena is larger than one body length; and (ii) the velocity component parallel to the fly body is larger than perpendicular to the body. 
Then we required (iii) a snippet duration of at least 20 frames (133 ms; roughly corresponds to one fly step); 
and (iv) a ratio of mean leg-tip speed to mean CoM speed smaller than 1.5. 
This produced a set of trajectory snippets with flies walking at different speeds, turning and standing still. 
The average walking speeds per snippet are distributed approximately in the range [0, 4] cm s$ ^{-1} $ (Fig.~\ref{fig:fig_3}, inset). 
We then linearly interpolated the walking snippets from 6.7-ms time steps to the walking simulation control of 2-ms time steps.


The 13 key points tracked in 2 dimensions, however, are not sufficient for the RL task reward calculation (Supplementary Information), which requires complete specification of the model’s pose, position and orientation. 
Specifically, this full-body representation should include all joint angles, joint axis orientations, joint velocities, body position and orientation and leg-tip positions. 
Obtaining the full-body data for the model from the experimental data required, first, lifting the 2D walking snippets by complementing the horizontal $ x $, $ y $ coordinates with the third vertical $ z $ dimension.
Based on a separate side-view video of a walking fly71, and our fly model’s default standing position, we approximated the body height and pitch angle during walking by a single fixed value. 
From this video, we also estimated the amplitudes of the arcs traversed by leg tips during swing motion. 
The amplitudes were A = 0.086, 0.047 and 0.051 cm for the T1, T2 and T3 legs, respectively. 
We approximated the z coordinate of the leg-tip swing arcs by the sine function as $ z = A sin(x) $, with $ x $ going from 0 to $ \pi $ for each single leg swing.
Using the 2D coordinates of the leg-tip key points, we separated leg swings from stances based on leg-tip horizontal velocities in the fly’s egocentric reference frame. 
Then we added the approximate sine-arcs to the swing segments of the leg-tip trajectories, keeping $ z = 0 $ for stances.
This procedure produced 3D coordinates for the 13 key points in the walking snippets selected earlier.


As a final step, we computed the full-body reference poses for each frame in all snippets. 
We added 13 key-point sites to the fly model and performed inverse kinematics fitting of the whole model body to the fly poses in the 3D walking snippets. 
For each snippet, we rescaled the reference key points to match the size of the fly model. 
In each frame, we simultaneously fit all 13 key points by minimizing the following objective with respect to the model joint angles, $ q = (q_1, q_2, ...) $:
\begin{equation}\label{key}
	\min_{q} [ 
		\sum_{i=1}^{13} || s_i (q) - s_i^{*} ||^2
		+ \lambda || q - q_0 ||^2
	 ]
\end{equation}
%
where $ s_i(q) $ and $ s_i^{*} $ are the 3D Cartesian coordinates of the 13 key points of the model and the fitting target pose, respectively. 
We used gradient descent to minimize this objective. 
To use the time continuity across frames, we used the final pose fitted for the previous frame to initialize the fitting procedure for each subsequent frame. 
Because the 13 key points (only 6 of which specify the leg tips) do not fully define the leg postures in space, we added a small regularization term to encourage fitting poses that are closer to the default standing pose of the fly model. 
The default pose is specified by a vector of model joint angles, $ q_0 $, a vector of zeros in our case. 
The regularization strength is $ \lambda = 1 \times 10^{-4} $ cm$ ^2 $ rad$ ^2 $.
Having found the joint angles q for the reference poses in each frame, we also computed joint velocities, $ dq/dt $, using finite differences.


This procedure resulted in a complete full-body representation (joint angles, positions, orientations and velocities) of the reference walking trajectories for the walking imitation task. 
In total, the walking dataset comprises around  16,000 walking snippets, amounting to around 80 min of fly walking behaviour.
The dataset is available at Figshare\cite{vaxenburg2025whole}.


% 触觉（搬箱子）
\textbf{Adhesion, friction and contact forces}. 
The fly model's ability to attach to and walk on inclined surfaces is enabled by the combination of adhesion, friction and contact forces. 
In this section we describe the details of the adhesion mechanism in MuJoCo and how the fly model uses the leg adhesion actuators. 
Let us consider a simple example of the stationary fly on an inclined plane. 
When a contact is detected between a tarsal claw collision geom and the floor surface, MuJoCo computes the (constraint) contact force, which is the ground reaction force in this case (Extended Data Fig.~\ref{fig:extended_fig_3}a). 
Within the Coulomb friction model, as long as the contact force vector $ f_\text{contact} $ is within the friction cone boundaries, the tangential component of the net external force ($ f_\text{weight} $, the fraction of the total fly weight supported by the given leg in this simplified example), which acts to produce slipping motion, will be balanced by the tangential component $ f_\text{contact} $ of the contact force in the opposite direction. 
The friction cone (our fly model uses elliptic friction cones; 
MuJoCo also supports pyramidal friction cones) includes all contact force vectors satisfying $ f_{\text{contact}^{||} \let \mu f_{\text{contact}^{-}}} $, where $ \mu $ is the static friction coefficient.
Outside of the friction cone, that is, for contact forces beyond the threshold $ \mu f_{\text{contact}}^{-} $, slipping motion will occur.
Note that within this friction model, the cone angle is a function of the friction coefficient alone and is given by $ \theta = tan^{-1} \mu $.
In our model, $ \mu = 1 $ and $ \theta = 45^{\circ} $.


% 粘附执行器
In MuJoCo, the action of an adhesion actuator is equivalent to injecting force in the normal contact direction (Extended Data Fig.~\ref{fig:fig_3}a), effectively acting to push the fly’s claw into the floor. 
In response, the normal component $ f_\text{contact}^{-} $ of the contact force will increase by the same amount. 
Although there is no change in the tangential contact force component owing to the adhesion, note how the net contact force vector is now further away from the friction cone boundary, thus providing a larger slip-resisting margin, which can be used for forward walking propulsion, for example. 
Beyond its role in the Coulomb friction mechanism, the adhesion force can also directly counteract gravity to enable walking on arbitrarily oriented surfaces, such as vertical walls or ceilings.


% 行走
Extended Data Figure~\ref{fig:extended_fig_3}b shows the time-lapse of the walking task in which the fly model was trained to use adhesion to overcome bumpy terrain. 
This task is similar to the walking imitation task described above (Fig.~\ref{fig:fig_3}). 
The fly is required to imitate a single real-data walking snippet of walking straight at a fixed speed, 2.7 cm s$ ^{-1} $.
In the fly’s way, we introduced a sine-like bump obstacle that cannot be overcome without adhesion. 
The bump obstacle is procedurally regenerated at each training episode with the bump’s height and length varying in the ranges [0, 2] and [2, 4] cm, respectively.
Thus, the bump inclination angle was between 0° and 72°. 
We also added a small action penalty, epsilon\_penalty=$ 3\times 10^{-4} $, through the DMPO agent mechanism, to encourage the agent to prefer economic actions, including the adhesion action. 
We recorded the adhesion action and contact forces during a trained policy rollout on a bump with a maximum inclination angle of around 45$ ^\circ $°, as shown in Extended Data Fig.~\ref{fig:extended_fig_3}c–f.


The adhesion forces produced by the leg adhesion actuators while overcoming the obstacle are shown in Extended Data Fig.~\ref{fig:extended_fig_3}c. 
The model’s use of adhesion increases as the terrain angle becomes steeper. 
The fly model learnt to use mostly the T1 and T2 leg pairs on the way uphill, and mostly T3 on the way downhill. 
Owing to the lack of constraints, there is an asymmetry (degeneracy) between the left and right leg adhesion use, which we did not attempt to resolve. 
In our model, the largest adhesion force per leg is one fly body weight, which is also shown in the figure for comparison. The norm of the corresponding contact force vectors $ | f_\text{contact} | $ for each leg is shown in Extended Data Fig.~\ref{fig:extended_fig_3}d. 
The effect of creating a larger slip-resisting margin—moving fcontact further away from the friction cone boundary—with increasing adhesion is shown in Extended Data Fig.~\ref{fig:extended_fig_3}e. 
Without the adhesion, most of the leg-floor contacts would not have been able to counteract the slipping force load, especially in the ‘driving’ T1, T2 legs on the way up, and T3 on the way down (Extended Data Fig.~\ref{fig:extended_fig_3}f).



\subsection{Data availability} \label{sec:data_availability}

Confocal imaging stack, flight and walking imitation datasets, base wing-beat pattern, grooming pose data, and trained controller networks are available at Figshare\cite{vaxenburg2025whole}.


\subsection{Code availability} \label{sec:code_availability}

The fly model and code are publicly available at https://github.com/TuragaLab/flybody


\section{Extended Figure}

\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_1.pdf}
	\caption{
	} \label{fig:extended_fig_1}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_2.pdf}
	\caption{
	} \label{fig:extended_fig_2}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_3.pdf}
	\caption{}
	\label{fig:extended_fig_3}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_4.pdf}
	\caption{}
	\label{fig:extended_fig_4}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_5.pdf}
	\caption{}
	\label{fig:extended_fig_5}
\end{figure}



\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_6.pdf}
	\caption{}
	\label{fig:extended_fig_6}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_7.pdf}
	\caption{}
	\label{fig:extended_fig_7}
\end{figure}


\begin{figure}[!htb] 
	\centering
	\includegraphics[width=0.9\textwidth]{fig/extended_fig_8.pdf}
	\caption{}
	\label{fig:extended_fig_8}
\end{figure}





\section{Conclusion}\label{sec13}

Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}
	
	
\section{Supplementary information} \label{secInfo}

\subsection{Supplementary Methods}

\subsubsection{Constructing the physics fly model in MuJoCo}

Here we describe the steps taken in order to create the physics MuJoCo fly model, given the geometrical Blender model and measured masses of fly body parts. 
The resulting MuJoCo model is available at \href{https://github.com/TuragaLab/flybody}{https://github.com/TuragaLab/flybody}.


% OpenSim -> MuJoCo plugin
\textbf{Initial conversion}. The Blender-to-MuJoCo export plug-in\cite{tunyasuvunakool2020dm_control}1 was used to export a raw MuJoCo model containing only geometrical information: 
% 仅包含几何信息：带关节轴和限制的 身体网格 和 运动学树
body meshes and a kinematic tree with joint axes and limits.


\textbf{Model building script}. 
% 使用PyMJCF操作导入 MuJoCo 的原始模型
The raw MuJoCo model was then loaded and manipulated with a Python script using PyMJCF, a Python library for model manipulation which is part of Google DeepMind’s dm control suite\cite{tunyasuvunakool2020dm_control}. 
The following steps were taken.


Enforced consistent naming everywhere using a part sternum side convention e.g., “coxa T3 right”. 
Consistent naming allows for conveniently readable loops like


% 流体作用力的现象学模型
\subsubsection{Phenomenological model of fluid forces}


% 库塔升力
\textbf{Kutta lift}

% 马格努斯力是旋转物体在流体中运动时因压力差产生的横向作用力，其方向垂直于流体流动方向与物体旋转轴。
\textbf{Magnus force}


% 粘性阻力是船舶航行时因流体粘性产生的阻力，由摩擦阻力和粘性压差阻力组成，其中摩擦阻力占比超过50%
\textbf{Viscous drag}


% 附加质量
% 物体在流体中变速运动，推动物体的力不仅要为增加物体的动能做功，还要为增加周围流体的动能做功。因此具有一定质量的物体要获得加速度，施加在它上面的力将大于物体质量与加速度的乘积，增加的这部分质量就是附加质量。 [1]附加质量与物体本身的形状及运动方向有关。
\textbf{Added mass}


% 跑步模仿任务 的配置
\subsubsection{Running imitation task configuration}

% 跑步控制的再使用（视觉引导跑步任务配置）
\subsubsection{Running controller reuse, vision-guided running task configuration}


% 跑步模仿人物 的配置
\subsubsection{Walking imitation task configuration}


% 模拟速度
\subsubsection{Simulation speed}





\subsection{Supplementary table}\label{secA1}




% 多个人 各个部位的 实证质量
\textbf{Supplementary Table 1}: Empirical masses of the fly body parts. 
Averaged over 52 male humans.

\begin{table}[htbp]
	\centering
	\small
	\begin{tabular}{cc}
		\toprule
		Body part         &        Mass (mg)     \\
		\midrule
		head      &   0.15      \\
		throx      &   0.34      \\
		abdomen      &   0.38      \\
		leg (each)      &   0.0162      \\
		wing (each)      &   0.008      \\
		\midrule
		fly total      &   0.983      \\
		
		\bottomrule
	\end{tabular}%
	\label{tab:s_1}%
\end{table}%





\begin{table}[htbp]
	\centering
	\small
	\caption{% 感觉系统
		\textbf{Supplementary Table 3}: 
		Sensory system of the running model in its default configuration. 
		Task-specific model modifications (e.g., disabling hands for running) may include disabling some of the sensors and altering the dimensions of the proprioception observables. 
		All observables are calculated in human's egocentric reference frame. 
		The actuator activation state units are the same as the corresponding actuator control units.}
	\begin{tabular}{ccc}
		\toprule
		Actuator type         &        Located at  & \# of actoators     \\
		\midrule
		torque      &   wings      &      6  \\
		position      &   neck      &      6  \\
		      &   proboscis      &      3  \\
		      &   antennae      &      5  \\
		      &   legs (excluding tarsi)      &      42  \\
		\midrule
		position, coupled by tendon      &   tarsi      &      6  \\
		(several DoFs per actuator)      &   abdomen      &      2  \\
		\midrule
		adhension      &   leg tips      &      6  \\
		      &   labrum      &      2  \\
		\midrule
		total      &   wings      &      78  \\
		
		\bottomrule
	\end{tabular}%
	\label{tab:s_2}%
\end{table}%




\begin{table}[htbp]
	\centering
	\small
	\caption{% 感觉系统
		\textbf{Supplementary Table 3}: 
		Sensory system of the running model in its default configuration. 
		Task-specific model modifications (e.g., disabling hands for running) may include disabling some of the sensors and altering the dimensions of the proprioception observables. 
		All observables are calculated in human's egocentric reference frame. 
		The actuator activation state units are the same as the corresponding actuator control units.}
	\begin{tabular}{cccc}
		\toprule
		Category         &        Sensor  & Units  & Array shape   \\
		\midrule
		vision      &   right eye camera      &    unitless RGB & (32, 32, 3)  \\
		      &   left eye camera      &     & (32, 32, 3)  \\
		% 速度计（人的前庭系统中没有）
		vesitibular      &   velocimeter      &    cm/s & 3  \\
		      &   accelerometer      &    cm/s$ ^2 $ & 3  \\
		      &   gyro (rotational velocity)      &    rad/s & 3  \\
		      &   gravity direction      &    cm & 3  \\
		% 本体感受：关节角度（传感器是什么）
		proprioception      &   joint angles      &    rad & 102  \\
		      &   joint velocities      &    rad/s & 102  \\
		      &   actuator activation state      &    same as control & 78  \\
		% 末端器位置
		      &   end-effector positions      &    unitless RGB & (7, 3)  \\
		% 机械性感受
		mechanoreception      &   leg force sensors      &    dyn & (6, 3)  \\
		      &   leg touch sensors      &    dyn & 6  \\
		\bottomrule
	\end{tabular}%
	\label{tab:s_3}%
\end{table}%






\begin{table}[htbp]
	\centering
	\small
	\caption{\textbf{Supplementary Table 4}: 
		% 传感系统 的不同之处
		Correspondence between the sensory system components in human and in our human model.}
	\begin{tabular}{ccccc}
		\toprule
		\makecell{Human sense\\organ, receptor}         &        Sensory modality  & \makecell{Prupose, dete\\ction, sensing}  & \makecell{Corresponding human\\ model sensor}  & Reference \\
		\midrule
		% Looming stimuli（逼近刺激）指通过模拟物体或威胁源的逼近运动，引发动物或人类的本能反应
		Compound eyes     &   \makecell{Light,\\ photoreception}      &    \makecell{Self-move\\ment (optic \\flow, \\optomotor \\response), \\looming \\stimuli} & \makecell{Eye cameras, \\ velocimeter, \\ gravity \\ direction} & \cite{hengstenberg1993multisensory,strausfeld1985convergence} \\
		\midrule
		% 单眼
		Ocelli     &   \makecell{Light,\\ photoreception}      &    \makecell{Dorsal light\\ response,\\ body\\ orientation,\\ postural \\ correction} & As above & \cite{hengstenberg1993multisensory,strausfeld1985convergence,mimura1970convergence} \\
		\midrule
		% 果蝇稳定的飞行需要称为“halteres”的本体感觉器官来检测身体旋转
		% 单眼
		\makecell{Halteres \\(modified \\2nd wing pair)}     &   \makecell{Mechano-\\reception}      &   \makecell{Coriolis \\forces during\\ flight; \\balance/\\equilibrium \\organ} & \makecell{Gyro, \\accelerometer} & \cite{strausfeld1985convergence,fayyazuddin1996haltere} \\
		\midrule
		\makecell{Leg \\ campaniform\\sensilla}     &   \makecell{Mechano-\\reception}      &   \makecell{Load/gravity} & \makecell{Force, touch,\\ gravity\\ direction} & \cite{dinges2021location} \\
		\midrule
		% 江氏器（Johnston's organ）    是昆虫触角梗节中较常见的一种弦音感器（chordotonal sensillum），最早由Johnston于1855年在埃及伊蚊Aedes aegypti (L.)的雄蚊触角梗角中发现。江氏器主要用于昆虫的定向，如埃及伊蚊的雄蚊常藉400Hz～650Hz的音频来判断雌蚊的位置，找到雌蚊，进行交配
		\makecell{Johnston's\\ organ (2nd \\ antennal\\ segment)}     &   \makecell{Mechano-\\reception \\ (exteroception)}      &   \makecell{Gravity, \\air flow,\\sound (\\courtship\\ song)} & \makecell{gravity \\direction,\\ velocimiter\\, sound not \\implemented} & \cite{kamikouchi2009neural,mimura1970convergence,mamiya2015antennal} \\
		\midrule
		\makecell{Neck \\proprioceptor\\ (prosternal\\ organ, CO)}     &   \makecell{Mechano-\\reception \\ (proprioception)}      &   \makecell{head \\position\\; positural\\ correction} & \makecell{Joint \\angles, \\joint \\velocities} & \cite{strausfeld1985convergence} \\
		\midrule
		\makecell{Wing base \\tegula}     &   \makecell{Mechano-\\reception \\ via a hair \\plate and \\ campaniform\\ sensilla}      &   \makecell{Wing \\position (\\proprioception)} & As above & \cite{fudalewicz1963innervation} \\
		\bottomrule
	\end{tabular}%
	\label{tab:s_4}%
\end{table}%




% 运行时间分析
\begin{table}[htbp]
	\centering
	\small
	\caption{\textbf{Supplementary Table 5}: 
		Breakdown of average times of one control timestep for different fly model behaviors simulated on a single core of Intel Xeon CPU E5-2697 v3 @ 2.60GHz. 
		All times are in units of ms.}
	\begin{tabular}{ccccccccc}
		\toprule
		\makecell{Component:\\Behavior}         &        Policy  & RL env  & \makecell{MuJoCo\\control}  & \makecell{MuJoCo\\physics} & \makecell{Total\\step time} & \makecell{Real time\\simulated} & \makecell{\% real time\\total} & \makecell{\% real time\\total} \\
		\midrule
		Running     &   3.64      &    3.67 & 1.11 & 0.278 & 8.43 & 0.2 & 2.37\% & 18.0\% \\
		\midrule
		Walking     &   4.31      &    4.49 & 4.64 & 0.464 & 13.73 & 2.0 & 14.6\% & 43.1\% \\
		\makecell{Vision-\\guided\\flight}     &   4.51      &    9.83 & 1.03 & 0.257 & 15.36 & 0.2 & 1.30\% & 19.5\% \\
		\bottomrule
	\end{tabular}%
	\label{tab:s_5}%
\end{table}%




\begin{table}[htbp]
	\centering
	\small
	\caption{\textbf{Supplementary Table 6}: 
		Observations in the running imitation task. 
		All observables are calculated in fly’s egocentric reference frame.}
	\begin{tabular}{cccc}
		\toprule
		Category         &        Observables (policy inputs)  & Units  & Array shape  \\
		\midrule
		Vesitibular     &  velocimeter      &   cm/s & 3  \\
		     &  accelerameter      &   cm/s$ ^2 $ & 3  \\
		     &  gyro      &   rad/s$ ^2 $ & 3  \\
		     &  gravity direction      &   cm & 3  \\
		\midrule
		proprioception     &  joint angles      &   rad & 25  \\
		     &  joint velocities      &   rad/s & 25  \\
		\midrule
		task inputs     &  reference CoM Displacement      &   cm & (6, 3)  \\
		(steering commands)     &  reference orientation displacement      &   \makecell{unitless \\quaternion} & (6, 4)  \\
		\midrule
		total obs. dimension     &        &    & 104  \\
		\bottomrule
	\end{tabular}%
	\label{tab:s_6}%
\end{table}%



% 跑步模仿任务中 的动作
\begin{table}[htbp]
	\centering
	\caption{\textbf{Supplementary Table 7}: Action in the running imitation task.}
	\small
	\begin{tabular}{cc}
		\toprule
		Actions (policy outputs)         &        Array shape     \\
		\midrule
		head      &   3      \\
		wings      &   6      \\
		abdomen      &   2      \\
		WPG frequency control      &   1      \\
		\midrule
		total action dimension      &   12      \\
		\bottomrule
	\end{tabular}%
	\label{tab:s_7}%
\end{table}%


% 跑步中的观测量
\begin{table}[htbp]
	\centering
	\small
	\caption{\textbf{Supplementary Table 8}: 
		Observations in the walking imitation task. 
		All observables are calculated in human's egocentric reference frame.
		The actuator activation state units are the same as the corresponding actuator control units.}
	\begin{tabular}{cccc}
		\toprule
		Category         &        Observables (policy inputs)  & Units  & Array shape  \\
		\midrule
		Vesitibular     &  velocimeter      &   cm/s & 3  \\
		&  accelerameter      &   cm/s$ ^2 $ & 3  \\
		&  gyro      &   rad/s$ ^2 $ & 3  \\
		&  gravity direction      &   cm & 3  \\
		\midrule
		proprioception     &  joint angles      &   rad & 85  \\
		&  joint velocities      &   rad/s & 85  \\
		&  actuator activation state      &   same as control & 59  \\
		&  end-effector positions      &   cm & (7, 3)  \\
		\midrule
		mechanoreception     &  leg force sensors      &   dyn & (6, 3)  \\
		     &  leg touch sensors      &   dyn & 6  \\
		\midrule
		task inputs     &  reference CoM Displacement      &   cm & (65, 3)  \\
		(steering commands)     &  reference orientation displacement      &   \makecell{unitless \\quaternion} & (65, 4)  \\
		\midrule
		total obs. dimension     &        &    & 741  \\
		\bottomrule
	\end{tabular}%
	\label{tab:s_8}%
\end{table}%


\subsection{Supplementary Video 1} \label{sec:NFT}

% TODO 虚幻中的 3 个跑步视频
Running imitation. 
Examples of human model imitating real running behaviours: 
% 1. 跟踪一个真实的 急速勒马 轨迹（折返跑）
a saccade turning manoeuvre, 
% 2. 躲避 动作 （跑圈？）
an evasion manoeuvre, 
% 3. 以恒定速度直线水平跑
and straight horizontal running at constant speed (30 cm s$ ^{-1} $). 
% 由策略网络 调制的 跑步模式发生器
The leg motion is driven by wing-beat pattern generator modulated by policy network.


\subsection{Supplementary Video 2} \label{sec:sup_2}

% TODO: 走路行为模仿的 3 个示例（斜视图；俯视图、侧视图）
Walking imitation. 
Examples of human model imitating walking behaviours: 
% 1. 跟踪一个真实的走路轨迹
tracking a real walking trajectory with variable speed and direction, 
% 2. 以固定的速度走直线
walking straight at constant speed (2 cm $ ^{-1} $),
% 3. 以固定的速度向右转
and turning right at constant speed (2 cm s$ ^{-1} $) and yaw speed (130 $ ^{\circ} $ s $^{-1} $).
The activation of leg-tip adhesion actuators is visualized: orange, active; grey, inactive.


\subsubsection{Supplementary Video 3} \label{sec:sup_2_1}

% TODO: 人体的共焦成像
Confocal imaging of human body. 
Visualization of the confocal z-stacks of a single male human body parts (head, thorax with abdomen, wings and legs).


\subsubsection{Supplementary Video 4} \label{sec:sup_2_2}

% TODO Blender 身体模型
Blender body model. 
Animation of the geometric human model and its individual body parts assembled in Blender.

\subsubsection{Supplementary Video 5} \label{sec:sup_2_3}

% TODO （起跑）驱动的 PhysX 身体模型
PhysX body model with actuation. 
Degrees of freedom of the physics human model in Hutb. 
All degrees of freedom are actuated sequentially and traverse approximately 50\% of their corresponding joint ranges. 
Collisions are disabled in this video.







\subsection{Supplementary figure}\label{secS1}








%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%\bibliography{reference}


\end{document}
